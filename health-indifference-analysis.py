# -*- coding: utf-8 -*-
"""Health Indifference (Final Version, CV, Binary)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkv98fy_U6NC34N5kQi2AvZkVZwgo0HV
"""

# -*- coding: utf-8 -*-
"""
JASTIS Health Indifference Study - 5-fold CV Version
Execute each causal inference method with 5-fold CV and consider edges detected in 3+ folds as significant
Version: 15.0
"""

# ========================================
# 1. Environment Setup and Package Installation
# ========================================
import subprocess
import sys
import os
import time
import warnings
warnings.filterwarnings('ignore')

def install_packages():
    """Install required packages"""
    print("Installing required packages...")
    start_time = time.time()

    # Check Google Colab environment
    try:
        import google.colab
        IN_COLAB = True
        print("Google Colab environment detected")
    except ImportError:
        IN_COLAB = False
        print("Running in local environment")

    # Required package list
    required_packages = [
        'numpy',
        'pandas',
        'matplotlib>=3.5.0',
        'seaborn>=0.12.0',
        'scikit-learn>=1.0.0',
        'statsmodels>=0.13.0',
        'scipy>=1.9.0',
        'tqdm',
        'openpyxl>=3.0.0',
        'missingno>=0.5.0',
        'joblib>=1.1.0',
        'networkx>=2.8.0',
        'torch>=1.9.0',
        'gcastle',
        'dagma'
    ]

    # Install packages
    for package in required_packages:
        try:
            pkg_name = package.split('>=')[0].split('[')[0]
            __import__(pkg_name)
            print(f"✓ {pkg_name} (already installed)")
        except ImportError:
            try:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])
                print(f"✓ {pkg_name} (newly installed)")
            except Exception as e:
                print(f"⚠ Failed to install {package}: {e}")

    print(f"\nPackage installation completed ({time.time() - start_time:.1f} seconds)")

# Execute package installation
install_packages()

# ========================================
# 2. Library Import and Configuration
# ========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import KFold
import re
import gc
import json
from datetime import datetime
import shutil
from typing import Dict, List, Tuple, Optional, Union
from collections import defaultdict
import matplotlib.patches as patches
import concurrent.futures
from functools import partial

# Statistics related
from scipy.stats import chi2_contingency, ttest_ind, brunnermunzel, fisher_exact
import statsmodels.api as sm
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Visualization related
import missingno as msno
import networkx as nx
from joblib import Parallel, delayed
try:
    from tqdm.notebook import tqdm
except ImportError:
    from tqdm import tqdm
import multiprocessing as mp
import torch

# gCastle package import
try:
    import castle
    from castle.algorithms import CORL, DirectLiNGAM, GOLEM
    from castle.metrics import MetricsDAG
    GCASTLE_AVAILABLE = True
    print("✓ gCastle available")
except Exception as e:
    GCASTLE_AVAILABLE = False
    print(f"⚠ gCastle not available: {e}")

# DAGMA package import
try:
    from dagma import utils
    from dagma.linear import DagmaLinear
    from dagma.nonlinear import DagmaMLP, DagmaNonlinear
    DAGMA_AVAILABLE = True
    print("✓ DAGMA available")
except Exception as e:
    DAGMA_AVAILABLE = False
    print(f"⚠ DAGMA not available: {e}")

# Google Colab detection and Drive mount
try:
    import google.colab
    IN_COLAB = True
    print("\nGoogle Colab environment detected")

    from google.colab import drive
    drive.mount('/content/drive')
    DRIVE_MOUNTED = True
    print("✓ Google Drive mounted")

    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/JASTIS_Analysis_Results'
    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)
    print(f"✓ Output directory: {DRIVE_OUTPUT_DIR}")
except:
    IN_COLAB = False
    DRIVE_MOUNTED = False
    DRIVE_OUTPUT_DIR = None
    print("Running in local environment")

# Environment settings
os.environ['OMP_NUM_THREADS'] = '4'
os.environ['MKL_NUM_THREADS'] = '4'

# PyTorch device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"✓ Device: {device}")

# Visualization settings
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3
sns.set_style("whitegrid")

#from google.colab import files
#uploaded = files.upload()

import pandas as pd
import numpy as np
from scipy.stats import chi2
import math
import warnings

# Handling DeprecationWarning:
#   - 'ignore'  silences the warning
#   - 'error'   raises it as an exception (recommended for debugging)
warnings.simplefilter("ignore", category=DeprecationWarning)

#  p-value formatter
def format_p(p_val: float, digits: int = 3, sci_threshold: float = 1e-6) -> str:
    """
    Utility that formats a p-value string according to publication style
    - If p < .001, returns 'p < .001'
    - If p is below sci_threshold, returns scientific notation, e.g. 'p = 3.2e-07'
    - Otherwise rounds to the specified number of decimal places
    """
    if p_val < 0.001:
        if p_val < sci_threshold:
            return f"p = {p_val:.1e}"
        else:
            return "p < .001"
    return f"p = {p_val:.{digits}f}"

#  Little's MCAR test function
def little_mcar_test(
    df: pd.DataFrame,
    max_vars: int = 50,
    min_pattern_size: int = 2
):
    """
    Little's MCAR χ² test (equivalent to BaylorEdPsych::LittleMCAR)

    Parameters
    ----------
    df : pandas.DataFrame
        Original data frame
    max_vars : int, default 50
        Upper limit on the number of columns used, to reduce computation
    min_pattern_size : int, default 2
        Ignore very rare missing-data patterns (< min_pattern_size rows)

    Returns
    -------
    chi2_total : float
    df_total   : int
    p_value    : float
    """
    work = pd.DataFrame(index=df.index)

    # Numeric columns → unchanged
    # Non-numeric columns → category codes (missing as NaN)
    for col in df.columns[:max_vars]:
        s = df[col]
        if np.issubdtype(s.dtype, np.number):
            work[col] = s
        else:
            work[col] = s.astype("category").cat.codes.replace(-1, np.nan)

    p = work.shape[1]
    mu = work.mean(skipna=True)
    Sigma = work.cov()

    chi2_total, df_total = 0.0, 0

    patterns = work.isna().apply(tuple, axis=1)
    for pat, idx in patterns.groupby(patterns).groups.items():
        if len(idx) < min_pattern_size:
            continue
        observed_cols = [i for i, m in enumerate(pat) if not m]
        if not observed_cols:             # Skip rows that are all missing
            continue

        cols = work.columns[observed_cols]
        sub = work.loc[idx, cols]
        mu_r = sub.mean()
        Sigma_r = Sigma.loc[cols, cols].to_numpy()

        # Moore–Penrose pseudo-inverse for singular covariance matrices
        Sigma_inv = np.linalg.pinv(Sigma_r)
        diff = (mu_r - mu[cols]).to_numpy().reshape(-1, 1)

        # Explicitly convert 0-D ndarray to scalar to avoid DeprecationWarning
        quadform = float((diff.T @ Sigma_inv @ diff).item())
        chi2_total += len(idx) * quadform
        df_total += len(cols)

    df_total -= p
    p_value = 1 - chi2.cdf(chi2_total, df_total)
    return chi2_total, df_total, p_value

#  Load data & perform the test
csv_path = "/content/Jastis_Analysis2.csv"  # change path as needed
df = pd.read_csv(csv_path)

chi2_stat, dof_stat, p_stat = little_mcar_test(df, max_vars=50)

print("Little's MCAR test:")
print(f"  χ² = {chi2_stat:,.1f}")
print(f"  df  = {dof_stat}")
print(f"  {format_p(p_stat)}")

#  Sample-size recalculation (RR = 0.80)
alpha   = 0.05
power   = 0.80
Z_alpha = 1.96   # two-sided
Z_beta  = 0.84   # 80 % power

p0 = 0.05        # Hospitalisation rate in low-indifference group
RR = 0.80
p1 = RR * p0

p_bar = (p0 + p1) / 2
numer = (Z_alpha * math.sqrt(2 * p_bar * (1 - p_bar)) +
         Z_beta  * math.sqrt(p0 * (1 - p0) + p1 * (1 - p1))) ** 2
denom = (p0 - p1) ** 2

n_per_group = math.ceil(numer / denom)
total_n = n_per_group * 2

print("\nSample size (RR = 0.80):")
print(f"  per group : {n_per_group:,}")
print(f"  total     : {total_n:,}")

# ========================================
# 3. Data Validation and Memory Optimization
# ========================================
class DataValidator:
    """Data integrity validation"""

    @staticmethod
    def check_age_consistency(df: pd.DataFrame) -> List[str]:
        """Check age consistency"""
        issues = []
        if 'Age_21' in df.columns and 'Age_23' in df.columns:
            mask = df['Age_21'].notna() & df['Age_23'].notna()
            age_diff = df.loc[mask, 'Age_23'] - df.loc[mask, 'Age_21']
            invalid_age = df.loc[mask][(age_diff < 1) | (age_diff > 3)]
            if len(invalid_age) > 0:
                issues.append(f"{len(invalid_age)} cases with invalid age progression")
        return issues

    @staticmethod
    def check_data_ranges(df: pd.DataFrame, var_ranges: Dict) -> List[str]:
        """Check variable ranges"""
        issues = []
        for var, (min_val, max_val) in var_ranges.items():
            if var in df.columns:
                valid_data = df[df[var].notna()]
                out_of_range = valid_data[(valid_data[var] < min_val) | (valid_data[var] > max_val)]
                if len(out_of_range) > 0:
                    issues.append(f"{var}: {len(out_of_range)} values out of range [{min_val}, {max_val}]")
        return issues

class MemoryOptimizer:
    """Memory usage optimization"""

    @staticmethod
    def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
        """Optimize data types to reduce memory usage"""
        df = df.copy()

        for col in df.columns:
            col_type = df[col].dtype

            if col_type != 'object':
                try:
                    c_min = df[col].min()
                    c_max = df[col].max()

                    if str(col_type)[:3] == 'int':
                        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                            df[col] = df[col].astype(np.int8)
                        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                            df[col] = df[col].astype(np.int16)
                        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                            df[col] = df[col].astype(np.int32)
                    else:
                        if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                            df[col] = df[col].astype(np.float32)
                except:
                    pass

        return df

# ========================================
# 4. Data Loading and Preprocessing
# ========================================
def load_and_validate_data(filepath: str) -> pd.DataFrame:
    """Load and validate data"""
    print("Loading data...")

    try:
        dtypes = {
            'UserID': 'int32',
            'Age_21': 'float32',
            'Age_22': 'float32',
            'Age_23': 'float32',
            'Sex_21': 'float32',
            'Education_21': 'float32',
            'HouseholdIncome_21': 'float32'
        }
        df = pd.read_csv(filepath, dtype=dtypes, low_memory=False)
    except:
        df = pd.read_csv(filepath, low_memory=False)

    df = MemoryOptimizer.optimize_dtypes(df)

    validator = DataValidator()
    issues = validator.check_age_consistency(df)

    if issues:
        print("Issues found in data validation:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("Data validation passed")

    print(f"Data loaded: {df.shape[0]:,} rows, {df.shape[1]:,} columns")
    print(f"Memory usage: {df.memory_usage().sum() / 1024**2:.1f} MB")

    return df

# ========================================
# 5. Health Indifference Score Calculation
# ========================================
class HealthIndifferenceCalculator:
    """Health indifference score calculation and validation"""

    def __init__(self):
        self.positive_items = [
            'HealthConscious_22', 'HealthInfoInterest_22', 'MonitorHealthChanges_22',
            'HealthAwareCompared_22', 'AcceptHealthSpending_22', 'DoAnythingForHealth_22',
            'SecureTimeForHealth_22', 'WantHealthyLife_22', 'ConcernForMaintenance_22'
        ]

        self.negative_items = [
            'IncomePriority_22', 'WorryOnlyWhenIll_22',
            'HobbyPriority_22', 'TreatmentOverPrevention_22'
        ]

        self.all_items = self.positive_items + self.negative_items
        self.cronbachs_alpha = None

    def calculate_cronbachs_alpha(self, df: pd.DataFrame) -> float:
        """Calculate Cronbach's alpha"""
        available_positive = [item for item in self.positive_items if item in df.columns]
        available_negative = [item for item in self.negative_items if item in df.columns]

        if not available_positive and not available_negative:
            return np.nan

        temp_df = df.copy()

        # Reverse positive items
        for item in available_positive:
            temp_df[f"{item}_reversed"] = 5 - temp_df[item]

        scored_items = [f"{item}_reversed" for item in available_positive] + available_negative
        items_data = temp_df[scored_items].dropna()

        if len(items_data) < 2:
            return np.nan

        k = len(scored_items)
        cov_matrix = items_data.cov()
        total_var = cov_matrix.values.sum()
        item_vars = cov_matrix.values.diagonal().sum()

        if total_var == 0:
            return np.nan

        alpha = (k / (k - 1)) * (1 - item_vars / total_var)
        return alpha

    def calculate_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate health indifference score"""
        df = df.copy()

        available_positive = [item for item in self.positive_items if item in df.columns]
        available_negative = [item for item in self.negative_items if item in df.columns]

        if not available_positive and not available_negative:
            print("Warning: Health indifference items not found in data")
            if 'HealthOrientationScore_22' in df.columns:
                df['HealthIndifferenceScore'] = df['HealthOrientationScore_22']
            else:
                df['HealthIndifferenceScore'] = np.nan
            df['HI_category'] = pd.Categorical([np.nan] * len(df))
            return df

        # Calculate Cronbach's alpha
        self.cronbachs_alpha = self.calculate_cronbachs_alpha(df)
        if not np.isnan(self.cronbachs_alpha):
            print(f"\nCronbach's α = {self.cronbachs_alpha:.3f}")

        # Score calculation
        for item in available_positive:
            df[f"{item}_reversed"] = 5 - df[item]

        scored_items = [f"{item}_reversed" for item in available_positive] + available_negative
        df['hi_valid_items'] = df[scored_items].notna().sum(axis=1)
        df['hi_score_calculated'] = np.nan

        valid_mask = df['hi_valid_items'] >= 10
        if valid_mask.any():
            df.loc[valid_mask, 'hi_score_calculated'] = df.loc[valid_mask, scored_items].mean(axis=1) * 13

        if 'HealthOrientationScore_22' in df.columns:
            df['HealthIndifferenceScore'] = df['HealthOrientationScore_22'].fillna(df['hi_score_calculated'])
        else:
            df['HealthIndifferenceScore'] = df['hi_score_calculated']

        # Create categories
        if df['HealthIndifferenceScore'].notna().any():
            median_score = df['HealthIndifferenceScore'].median()
            df['HI_category'] = pd.cut(
                df['HealthIndifferenceScore'],
                bins=[-np.inf, median_score, np.inf],
                labels=['Low HI', 'High HI']
            )

            # Also create binary variable
            df['High_HI'] = (df['HI_category'] == 'High HI').astype(int)

            print(f"\nHealth Indifference Score Statistics:")
            print(f"  Mean: {df['HealthIndifferenceScore'].mean():.2f}")
            print(f"  SD: {df['HealthIndifferenceScore'].std():.2f}")
            print(f"  Median: {median_score:.0f}")
            print(f"  Valid scores: {valid_mask.sum():,} ({valid_mask.sum()/len(df)*100:.1f}%)")
            print(f"  Low HI: {(df['HI_category']=='Low HI').sum():,}")
            print(f"  High HI: {(df['HI_category']=='High HI').sum():,}")

        return df

# ========================================
# 6. Variable Creation
# ========================================
class VariableCreator:
    """Create all variables based on research protocol"""

    @staticmethod
    def create_all_variables(df: pd.DataFrame) -> pd.DataFrame:
        """Create all analysis variables"""
        df = df.copy()

        # Sex
        if 'Sex_21' in df.columns:
            unique_vals = df['Sex_21'].dropna().unique()
            if set(unique_vals).issubset({1, 2}):
                df['Male'] = (df['Sex_21'] == 1).astype(int)
            elif set(unique_vals).issubset({0, 1}):
                df['Male'] = df['Sex_21'].astype(int)
            else:
                df['Male'] = (df['Sex_21'] == 1).astype(int)

        # Other 2021 variables
        df['Low_education'] = (df['Education_21'] == 1).astype(int) if 'Education_21' in df.columns else np.nan
        df['Low_income'] = (df['HouseholdIncome_21'] <= 3).astype(int) if 'HouseholdIncome_21' in df.columns else np.nan
        df['Living_alone'] = (df['HouseholdMembers_21'] == 1).astype(int) if 'HouseholdMembers_21' in df.columns else np.nan
        df['Lack_social_support'] = (df['SocialIsolation_21'] <= 3).astype(int) if 'SocialIsolation_21' in df.columns else np.nan

        if 'SmokingStatus_21' in df.columns:
            df['Current_smoking'] = df['SmokingStatus_21'].isin([1, 2]).astype(int)

        if 'DrinkingFrequency_21' in df.columns and 'DrinkingAmount_21' in df.columns:
            df['Heavy_drinking'] = ((df['DrinkingFrequency_21'] <= 4) &
                                   (df['DrinkingAmount_21'] == 5)).astype(int)

        if 'WalkingTimePerDay_21' in df.columns and 'VigorousExerciseTimePerDay_21' in df.columns:
            df['Low_physical_activity'] = ((df['WalkingTimePerDay_21'] <= 2) &
                                          (df['VigorousExerciseTimePerDay_21'] <= 2)).astype(int)

        # Health checkup
        if 'HealthCheckup_21' in df.columns:
            unique_vals = df['HealthCheckup_21'].dropna().unique()
            if set(unique_vals).issubset({1, 2}):
                df['No_health_checkup'] = (df['HealthCheckup_21'] == 2).astype(int)
            elif set(unique_vals).issubset({0, 1}):
                df['No_health_checkup'] = (df['HealthCheckup_21'] == 0).astype(int)
            else:
                df['No_health_checkup'] = (df['HealthCheckup_21'] == 1).astype(int)

        # BMI calculation
        if 'Weight_21' in df.columns and 'Height_21' in df.columns:
            df['BMI_21'] = df['Weight_21'] / (df['Height_21'] / 100) ** 2
            df['BMI_21'] = df['BMI_21'].clip(10, 60)

        # Self-rated health
        if 'SelfRatedHealth_21' in df.columns:
            df['Poor_self_rated_health'] = (df['SelfRatedHealth_21'] >= 4).astype(int)

        # Symptom count
        symptom_present_vars = [f'Symptom{i:02d}_Present_21' for i in range(1, 11)]
        available_symptoms = [col for col in symptom_present_vars if col in df.columns]
        if available_symptoms:
            df['Symptom_count_21'] = df[available_symptoms].apply(
                lambda row: (row == 1.0).sum(), axis=1
            )

        # Disease count
        chronic_disease_vars = [f'ChronicDisease{i:02d}_21' for i in range(1, 21)]
        available_diseases = [col for col in chronic_disease_vars if col in df.columns]
        if available_diseases:
            df['Disease_count_21'] = df[available_diseases].apply(
                lambda row: ((row == 3.0) | (row == 4.0)).sum(), axis=1
            )

        # Hospitalization history
        if 'HospitalizationPastYear_21' in df.columns:
            unique_vals = df['HospitalizationPastYear_21'].dropna().unique()
            if set(unique_vals).issubset({1, 2}):
                df['HospitalizationPastYear_21'] = (df['HospitalizationPastYear_21'] == 1).astype(int)

        # 2023 symptom variables
        symptom_vars_23 = [
            'DigestiveDisorder_23', 'BackPain_23', 'JointPain_23',
            'Headache_23', 'ChestPain_23', 'Dyspnea_23', 'Dizziness_23',
            'Fatigue_23', 'SleepDisorder_23', 'MemoryImpairment_23',
            'ConcentrationDecline_23', 'ReducedLibido_23','Cough_23', 'Fever_23'
        ]

        for var in symptom_vars_23:
            if var in df.columns:
                df[f'{var}_moderate'] = (df[var] >= 3).astype(int)

        # 2023 disease variables
        disease_vars_23 = [
            'Hypertension_23', 'Diabetes_23', 'Dyslipidemia_23',
            'Depression_23', 'Periodontitis_23', 'Asthma_23',
            'ChronicObstructivePulmonaryDisease_23', 'ChronicKidneyDisease_23',
            'DentalCaries_23', 'AnginaOrMI_23','Stroke_23',
            'ChronicLiverDisease_23','Cancer_23', 'ChronicPain_23',
        ]

        for var in disease_vars_23:
            if var in df.columns:
                df[f'{var}_current'] = df[var].isin([3, 4, 5]).astype(int)

        # 2023 hospitalization
        if 'HospitalizationPastYear_23' in df.columns:
            unique_vals = df['HospitalizationPastYear_23'].dropna().unique()
            if set(unique_vals).issubset({1, 2}):
                df['HospitalizationPastYear_23'] = (df['HospitalizationPastYear_23'] == 1).astype(int)

        print("✓ All variables created")

        return df

# ========================================
# 7. Missing Data Handling
# ========================================
class MissingDataHandler:
    """Missing data handling with multiple imputation"""

    def __init__(self, n_imputations: int = 20, random_state: int = 12345):
        self.n_imputations = n_imputations
        self.random_state = random_state

    def analyze_missing_patterns(self, df: pd.DataFrame) -> Dict:
        """Analyze missing data patterns"""
        missing_summary = pd.DataFrame({
            'variable': df.columns,
            'n_missing': df.isnull().sum(),
            'pct_missing': (df.isnull().sum() / len(df) * 100).round(2)
        })
        missing_summary = missing_summary[missing_summary['n_missing'] > 0].sort_values('pct_missing', ascending=False)

        if len(missing_summary) > 0:
            plt.figure(figsize=(12, 8))
            msno.matrix(df)
            plt.title('Missing Data Pattern')
            plt.tight_layout()
            plt.savefig('missing_pattern_binary.png', dpi=300)
            plt.close()

        return {
            'summary': missing_summary,
            'total_complete_cases': df.dropna().shape[0],
            'pct_complete_cases': (df.dropna().shape[0] / len(df) * 100) if len(df) > 0 else 0
        }

    def perform_mice(self, df: pd.DataFrame, vars_to_impute: List[str]) -> List[pd.DataFrame]:
        """Perform MICE (Multiple Imputation by Chained Equations)"""
        print(f"Running multiple imputation ({self.n_imputations} times)...")

        impute_data = df[vars_to_impute].copy()
        imputed_datasets = []

        for i in tqdm(range(self.n_imputations), desc="Imputing"):
            random_state = self.random_state + i

            imputer = IterativeImputer(
                random_state=random_state,
                max_iter=10,
                sample_posterior=True
            )

            try:
                imputed_array = imputer.fit_transform(impute_data)
                imputed_df = df.copy()
                imputed_df[vars_to_impute] = imputed_array
                imputed_datasets.append(imputed_df)
            except Exception as e:
                print(f"Warning: Error in imputation {i+1}: {e}")
                imputed_df = df.copy()
                for var in vars_to_impute:
                    if var in imputed_df.columns:
                        imputed_df[var].fillna(imputed_df[var].mean(), inplace=True)
                imputed_datasets.append(imputed_df)

        return imputed_datasets

    @staticmethod
    def combine_mi_results(results: List[Dict]) -> Dict:
        """Combine multiple imputation results using Rubin's rules"""
        if not results:
            return None

        m = len(results)

        estimates = np.array([r['estimate'] for r in results])
        ses = np.array([r['se'] for r in results])

        combined_estimate = np.mean(estimates)
        within_var = np.mean(ses**2)
        between_var = np.var(estimates, ddof=1) if m > 1 else 0
        total_var = within_var + (1 + 1/m) * between_var
        combined_se = np.sqrt(total_var)

        if between_var > 0:
            df = (m - 1) * (1 + within_var / ((1 + 1/m) * between_var))**2
        else:
            df = float('inf')

        t_critical = stats.t.ppf(0.975, df) if df < float('inf') else 1.96
        ci_lower = combined_estimate - t_critical * combined_se
        ci_upper = combined_estimate + t_critical * combined_se

        if combined_se > 0:
            t_stat = combined_estimate / combined_se
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))
        else:
            p_value = np.nan

        return {
            'estimate': combined_estimate,
            'se': combined_se,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'p_value': p_value,
            'df': df
        }

# ========================================
# 8. Statistical Analysis Functions
# ========================================
class StatisticalAnalyzer:
    """Statistical analysis execution"""

    @staticmethod
    def calculate_smd(group1: pd.Series, group2: pd.Series) -> float:
        """Calculate Standardized Mean Difference (SMD)"""
        group1 = group1.dropna()
        group2 = group2.dropna()

        if len(group1) == 0 or len(group2) == 0:
            return np.nan

        n1, n2 = len(group1), len(group2)
        mean1, mean2 = group1.mean(), group2.mean()
        var1, var2 = group1.var(), group2.var()

        pooled_sd = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))

        if pooled_sd == 0:
            return 0

        return abs(mean2 - mean1) / pooled_sd

    @staticmethod
    def is_count_variable(var_name: str) -> bool:
        """Determine if variable is a count variable based on name"""
        count_keywords = ['count', 'number', 'num_', 'n_', 'total_']
        return any(keyword in var_name.lower() for keyword in count_keywords)

    @staticmethod
    def create_table1(df: pd.DataFrame) -> pd.DataFrame:
        """Create Table 1: Participant characteristics"""
        results = []

        if 'HI_category' not in df.columns:
            print("Warning: HI_category not found")
            return pd.DataFrame()

        # Define all variables
        all_variables = [
            ('HealthIndifferenceScore', 'HI score, mean (SD)', 'continuous'),
            ('Age_21', 'Age (2021), years, mean (SD)', 'continuous'),
            ('Male', 'Male sex, n (%)', 'binary'),
            ('Low_education', 'Low education (<HS), n (%)', 'binary'),
            ('Low_income', 'Low household income, n (%)', 'binary'),
            ('Living_alone', 'Living alone, n (%)', 'binary'),
            ('Lack_social_support', 'Lack of social support, n (%)', 'binary'),
            ('Current_smoking', 'Current smoking, n (%)', 'binary'),
            ('Heavy_drinking', 'Heavy drinking, n (%)', 'binary'),
            ('BMI_21', 'BMI (2021), kg/m², mean (SD)', 'continuous'),
            ('Low_physical_activity', 'Low physical activity, n (%)', 'binary'),
            ('No_health_checkup', 'No health checkup, n (%)', 'binary'),
            ('Poor_self_rated_health', 'Poor self-rated health, n (%)', 'binary'),
            ('HospitalizationPastYear_21', 'Hospitalization past year (2021), n (%)', 'binary'),

            # 2023 symptoms
            ('Fatigue_23_moderate', 'Fatigue (2023), n (%)', 'binary'),
            ('SleepDisorder_23_moderate', 'Sleep disturbance (2023), n (%)', 'binary'),
            ('BackPain_23_moderate', 'Back pain (2023), n (%)', 'binary'),
            ('DigestiveDisorder_23_moderate', 'GI discomfort (2023), n (%)', 'binary'),
            ('Headache_23_moderate', 'Headache (2023), n (%)', 'binary'),
            ('JointPain_23_moderate', 'Joint pain (2023), n (%)', 'binary'),
            ('ConcentrationDecline_23_moderate', 'Concentration decline (2023), n (%)', 'binary'),
            ('MemoryImpairment_23_moderate', 'Memory disorder (2023), n (%)', 'binary'),
            ('ChestPain_23_moderate', 'Chest pain (2023), n (%)', 'binary'),
            ('Cough_23_moderate', 'Cough (2023), n (%)', 'binary'),
            ('Dyspnea_23_moderate', 'Dyspnea (2023), n (%)', 'binary'),
            ('Dizziness_23_moderate', 'Dizziness (2023), n (%)', 'binary'),
            ('ReducedLibido_23_moderate', 'Reduced libido (2023), n (%)', 'binary'),
            ('Fever_23_moderate', 'Fever (2023), n (%)', 'binary'),

            # 2023 diseases
            ('Hypertension_23_current', 'Hypertension (2023), n (%)', 'binary'),
            ('AnginaOrMI_23_current', 'Angina/MI (2023), n (%)', 'binary'),
            ('Stroke_23_current', 'Stroke (2023), n (%)', 'binary'),
            ('Cancer_23_current', 'Cancer (2023), n (%)', 'binary'),
            ('Diabetes_23_current', 'Diabetes mellitus (2023), n (%)', 'binary'),
            ('Dyslipidemia_23_current', 'Dyslipidemia (2023), n (%)', 'binary'),
            ('Depression_23_current', 'Depression (2023), n (%)', 'binary'),
            ('Periodontitis_23_current', 'Periodontitis (2023), n (%)', 'binary'),
            ('DentalCaries_23_current', 'Dental caries (2023), n (%)', 'binary'),
            ('Asthma_23_current', 'Asthma (2023), n (%)', 'binary'),
            ('ChronicObstructivePulmonaryDisease_23_current', 'COPD (2023), n (%)', 'binary'),
            ('ChronicKidneyDisease_23_current', 'Chronic kidney disease (2023), n (%)', 'binary'),
            ('ChronicLiverDisease_23_current', 'Chronic liver disease (2023), n (%)', 'binary'),
            ('ChronicPain_23_current', 'Chronic pain (2023), n (%)', 'binary'),

            # Others
            ('HospitalizationPastYear_23', 'Hospitalization past year (2023), n (%)', 'binary'),
            ('COVID19_infection_within_1year_23', 'COVID-19 infection past year (2023), n (%)', 'binary'),
            ('COVID19_vaccinated_23', 'COVID-19 vaccination (2023), n (%)', 'binary'),
            ('Influenza_vaccinated_23', 'Influenza vaccination (2023), n (%)', 'binary'),
        ]

        low_hi = df[df['HI_category'] == 'Low HI']
        high_hi = df[df['HI_category'] == 'High HI']

        for var, label, var_type in all_variables:
            if var not in df.columns:
                continue

            try:
                # Check data type and values
                unique_vals = df[var].dropna().unique()
                n_unique = len(unique_vals)

                # Automatic type detection
                actual_type = var_type
                if n_unique == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                    actual_type = 'binary'
                elif (var_type == 'count' or
                      StatisticalAnalyzer.is_count_variable(var) or
                      (n_unique > 2 and n_unique < 20 and
                       all(isinstance(v, (int, float)) and v >= 0 and v == int(v)
                           for v in unique_vals if pd.notna(v)))):
                    actual_type = 'count'
                elif n_unique > 10:
                    actual_type = 'continuous'

                if actual_type == 'count':
                    # Count data
                    low_median = low_hi[var].median()
                    low_q1 = low_hi[var].quantile(0.25)
                    low_q3 = low_hi[var].quantile(0.75)

                    high_median = high_hi[var].median()
                    high_q1 = high_hi[var].quantile(0.25)
                    high_q3 = high_hi[var].quantile(0.75)

                    if len(low_hi[var].dropna()) > 0 and len(high_hi[var].dropna()) > 0:
                        _, p_value = brunnermunzel(low_hi[var].dropna(), high_hi[var].dropna(), alternative='two-sided')
                    else:
                        p_value = np.nan

                    smd = StatisticalAnalyzer.calculate_smd(low_hi[var], high_hi[var])

                    results.append({
                        'Variable': label,
                        f'Low HI (n={len(low_hi):,})': f"{low_median:.0f} ({low_q1:.0f}-{low_q3:.0f})",
                        f'High HI (n={len(high_hi):,})': f"{high_median:.0f} ({high_q1:.0f}-{high_q3:.0f})",
                        'SMD': f"{smd:.3f}" if not np.isnan(smd) else "NA",
                        'p-value': '<0.001' if p_value < 0.001 else f'{p_value:.3f}' if not np.isnan(p_value) else 'NA'
                    })

                elif actual_type == 'continuous':
                    # Continuous data
                    low_mean = low_hi[var].mean()
                    low_sd = low_hi[var].std()
                    high_mean = high_hi[var].mean()
                    high_sd = high_hi[var].std()

                    if len(low_hi[var].dropna()) > 0 and len(high_hi[var].dropna()) > 0:
                        _, p_value = ttest_ind(low_hi[var].dropna(), high_hi[var].dropna())
                    else:
                        p_value = np.nan

                    smd = StatisticalAnalyzer.calculate_smd(low_hi[var], high_hi[var])

                    results.append({
                        'Variable': label,
                        f'Low HI (n={len(low_hi):,})': f"{low_mean:.1f} ({low_sd:.1f})",
                        f'High HI (n={len(high_hi):,})': f"{high_mean:.1f} ({high_sd:.1f})",
                        'SMD': f"{smd:.3f}" if not np.isnan(smd) else "NA",
                        'p-value': '<0.001' if p_value < 0.001 else f'{p_value:.3f}' if not np.isnan(p_value) else 'NA'
                    })

                else:  # binary
                    # Binary data
                    low_n = int(low_hi[var].sum())
                    low_total = len(low_hi[var].dropna())
                    low_pct = (low_n / low_total * 100) if low_total > 0 else 0

                    high_n = int(high_hi[var].sum())
                    high_total = len(high_hi[var].dropna())
                    high_pct = (high_n / high_total * 100) if high_total > 0 else 0

                    # Chi-square test
                    if low_total > 0 and high_total > 0:
                        observed = np.array([[low_n, low_total - low_n],
                                           [high_n, high_total - high_n]])
                        if observed.min() >= 5:
                            _, p_value, _, _ = chi2_contingency(observed)
                        else:
                            _, p_value = fisher_exact(observed)
                    else:
                        p_value = np.nan

                    # SMD calculation
                    p1 = low_n / low_total if low_total > 0 else 0
                    p2 = high_n / high_total if high_total > 0 else 0
                    pooled_p = (p1 + p2) / 2
                    if pooled_p > 0 and pooled_p < 1:
                        smd = abs(p2 - p1) / np.sqrt(pooled_p * (1 - pooled_p))
                    else:
                        smd = 0

                    results.append({
                        'Variable': label,
                        f'Low HI (n={len(low_hi):,})': f"{low_n:,} ({low_pct:.1f})",
                        f'High HI (n={len(high_hi):,})': f"{high_n:,} ({high_pct:.1f})",
                        'SMD': f"{smd:.3f}",
                        'p-value': '<0.001' if p_value < 0.001 else f'{p_value:.3f}' if not np.isnan(p_value) else 'NA'
                    })

            except Exception as e:
                print(f"Error: Problem processing {var}: {e}")
                continue

        return pd.DataFrame(results)

# ========================================
# 9. Causal Inference Analysis (5-fold CV Version)
# ========================================
class CausalInferenceAnalyzer:
    """Integrated implementation of causal inference methods using gCastle (5-fold CV version)"""

    def __init__(self, random_state: int = 12345, hi_var_type: str = 'continuous', use_cv: bool = True, n_folds: int = 5):
        """
        Parameters
        ----------
        random_state : int
            Random seed
        hi_var_type : str
            'continuous': HealthIndifferenceScore (continuous value 13-52)
            'binary': High_HI (binary variable 0/1)
        use_cv : bool
            True: Execute 5-fold CV
            False: Traditional single execution
        n_folds : int
            Number of CV folds (default 5)
        """
        self.random_state = random_state
        self.hi_var_type = hi_var_type
        self.use_cv = use_cv
        self.n_folds = n_folds
        self.results = {}
        self.device = device

        # Logical constraints
        base_constraints = [
            # COVID infection → vaccination (logically reversed)
            ('COVID19_infection_within_1year_23', 'COVID19_vaccinated_23'),
            ('COVID19_infection_within_1year_23', 'Influenza_vaccinated_23'),
        ]

        # Knowledge ①: Hierarchy of socioeconomic factors
        # Layer definitions
        layer1_vars = ['Age_21', 'Male']
        layer2_vars = ['Low_education']
        layer3_vars = ['Low_income', 'Living_alone', 'Lack_social_support']
        layer4_vars = ['Current_smoking', 'Heavy_drinking', 'Low_physical_activity',
                       'BMI_21', 'Poor_self_rated_health', 'No_health_checkup',
                       'Disease_count_21', 'Symptom_count_21']

        hierarchical_constraints = []
        # Prohibit paths from lower to upper layers
        # L4 -> L3
        for source in layer4_vars:
            for target in layer3_vars:
                hierarchical_constraints.append((source, target))
        # L4 -> L2
        for source in layer4_vars:
            for target in layer2_vars:
                hierarchical_constraints.append((source, target))
        # L3 -> L2
        for source in layer3_vars:
            for target in layer2_vars:
                hierarchical_constraints.append((source, target))

        # Knowledge ②: Causal direction from health behaviors to health status
        behavior_vars = ['Current_smoking', 'Heavy_drinking', 'Low_physical_activity', 'No_health_checkup']
        condition_vars = ['BMI_21', 'Poor_self_rated_health', 'Disease_count_21', 'Symptom_count_21']

        behavior_to_condition_constraints = []
        # Prohibit paths from status → behavior
        for source in condition_vars:
            for target in behavior_vars:
                behavior_to_condition_constraints.append((source, target))

        # Knowledge ③: Comprehensive temporal and logical constraints redefinition
        immutable_vars = ['Age_21', 'Male', 'Low_education']
        vars_2021 = layer3_vars + layer4_vars
        vars_2022 = ['HealthIndifferenceScore', 'High_HI']
        vars_2023 = [
            'Hypertension_23_current', 'Diabetes_23_current', 'Dyslipidemia_23_current',
            'Depression_23_current', 'Periodontitis_23_current', 'Asthma_23_current',
            'ChronicObstructivePulmonaryDisease_23_current', 'ChronicKidneyDisease_23_current',
            'DentalCaries_23_current', 'AnginaOrMI_23_current','Stroke_23_current',
            'ChronicLiverDisease_23_current','Cancer_23_current', 'ChronicPain_23_current',
            'HospitalizationPastYear_23', 'COVID19_infection_within_1year_23',
            'COVID19_vaccinated_23', 'Influenza_vaccinated_23',
            'Fatigue_23_moderate', 'SleepDisorder_23_moderate', 'BackPain_23_moderate',
            'DigestiveDisorder_23_moderate', 'Headache_23_moderate', 'JointPain_23_moderate',
            'ConcentrationDecline_23_moderate', 'MemoryImpairment_23_moderate',
            'ChestPain_23_moderate', 'Dyspnea_23_moderate', 'Dizziness_23_moderate',
            'ReducedLibido_23_moderate', 'Cough_23_moderate', 'Fever_23_moderate',
        ]

        comprehensive_constraints = []
        # 1. Prohibit causality to immutable variables
        all_other_vars = vars_2021 + vars_2022 + vars_2023
        for source in all_other_vars:
            for target in immutable_vars:
                comprehensive_constraints.append((source, target))
        # Also prohibit causality between immutable variables
        comprehensive_constraints.extend([
            ('Male', 'Age_21'), ('Age_21', 'Male'),
            ('Low_education', 'Age_21'), ('Age_21', 'Low_education'),
            ('Low_education', 'Male'), ('Male', 'Low_education'),
        ])

        # 2. Prohibit causality from lower to upper layers (immutable variables) (L4,L3 -> L1)
        for source in layer3_vars + layer4_vars:
            for target in layer1_vars:
                comprehensive_constraints.append((source, target))

        # Integrate all constraints
        self.logical_constraints = list(set(
            base_constraints +
            hierarchical_constraints +
            behavior_to_condition_constraints +
            comprehensive_constraints
        ))

        print(f"HI variable type: {hi_var_type}")
        print(f"CV execution: {'Yes' if use_cv else 'No'} ({n_folds} folds)" if use_cv else "")
        print(f"Total logical constraints: {len(self.logical_constraints)}")

    def prepare_causal_data(self, df: pd.DataFrame, variable_subset: str = 'all') -> Tuple[np.ndarray, List[str]]:
        """Prepare data for causal inference (supporting HI variable types)"""
        print(f"\nPreprocessing policy ({variable_subset} group):")
        print("- Binary variables (0/1): No standardization (preserve interpretability)")
        print("- Continuous variables: Standardization with RobustScaler (robust to outliers)")
        print("- Count variables: Standardization with RobustScaler")
        print(f"- HI variable: {self.hi_var_type}")

        # Variable lists
        determinant_vars_2021 = ['Male', 'Low_education', 'Lack_social_support',
                                'Current_smoking', 'No_health_checkup', 'Low_income',
                                'Living_alone', 'Age_21', 'Poor_self_rated_health',
                                'Heavy_drinking', 'Low_physical_activity', 'BMI_21']

        # HI variable selection
        if self.hi_var_type == 'binary':
            hi_var_2022 = ['High_HI']
        else:
            hi_var_2022 = ['HealthIndifferenceScore']

        # 2023 outcomes
        disease_infection_vars_2023 = [
            'Hypertension_23_current', 'Diabetes_23_current', 'Dyslipidemia_23_current',
            'Depression_23_current', 'Periodontitis_23_current', 'Asthma_23_current',
            'ChronicObstructivePulmonaryDisease_23_current', 'ChronicKidneyDisease_23_current',
            'DentalCaries_23_current', 'AnginaOrMI_23_current','Stroke_23_current',
            'ChronicLiverDisease_23_current','Cancer_23_current', 'ChronicPain_23_current',
            'HospitalizationPastYear_23',
            'COVID19_infection_within_1year_23', 'COVID19_vaccinated_23', 'Influenza_vaccinated_23'
        ]

        symptoms_vars_2023 = [
            'Fatigue_23_moderate', 'SleepDisorder_23_moderate', 'BackPain_23_moderate',
            'DigestiveDisorder_23_moderate', 'Headache_23_moderate', 'JointPain_23_moderate',
            'ConcentrationDecline_23_moderate', 'MemoryImpairment_23_moderate',
            'ChestPain_23_moderate', 'Dyspnea_23_moderate', 'Dizziness_23_moderate',
            'ReducedLibido_23_moderate', 'Cough_23_moderate', 'Fever_23_moderate',
        ]

        # Select variables according to subset
        if variable_subset == 'disease_infection':
            outcome_vars_2023 = disease_infection_vars_2023
        elif variable_subset == 'symptoms':
            outcome_vars_2023 = symptoms_vars_2023
        else:
            outcome_vars_2023 = disease_infection_vars_2023 + symptoms_vars_2023

        all_vars = determinant_vars_2021 + hi_var_2022 + outcome_vars_2023
        available_vars = [v for v in all_vars if v in df.columns]

        # Remove missing values
        data_for_causal = df[available_vars].dropna()

        # Determine variable types
        binary_vars = []
        count_vars = []
        continuous_vars = []

        for var in available_vars:
            unique_values = data_for_causal[var].unique()
            n_unique = len(unique_values)

            if n_unique == 2 and set(unique_values).issubset({0, 1, 0.0, 1.0}):
                binary_vars.append(var)
            elif 'count' in var.lower() or (
                n_unique > 2 and
                all(isinstance(v, (int, float)) and v >= 0 and v == int(v) for v in unique_values if pd.notna(v))
                and max(unique_values) < 20
            ):
                count_vars.append(var)
            else:
                continuous_vars.append(var)

        # Standardization
        data_scaled = data_for_causal.copy()
        # Unify to float32 from the beginning
        data_scaled = data_scaled.astype(np.float32)
        vars_to_scale = continuous_vars + count_vars

        # Add HI variable if continuous and not already in scaling list
        if self.hi_var_type == 'continuous' and 'HealthIndifferenceScore' in available_vars:
            if 'HealthIndifferenceScore' not in vars_to_scale:
                vars_to_scale.append('HealthIndifferenceScore')

        if vars_to_scale:
            scaler = RobustScaler()
            # Make scaler work with float32
            scaled_values = scaler.fit_transform(data_for_causal[vars_to_scale].astype(np.float32))
            data_scaled[vars_to_scale] = scaled_values.astype(np.float32)

        # Return as float32
        data_array = data_scaled.values.astype(np.float32)
        return data_array, available_vars

    def classify_vars_by_year(self, var_names: List[str]) -> Dict[str, List[int]]:
        """Classify variables by year (supporting HI variable types)"""
        vars_by_year = {'2021': [], '2022': [], '2023': []}

        for i, var in enumerate(var_names):
            if var in ['HealthIndifferenceScore', 'High_HI']:
                vars_by_year['2022'].append(i)
            elif any(suffix in var for suffix in ['_23', '_current', '_moderate', 'PastYear_23']):
                vars_by_year['2023'].append(i)
            else:
                vars_by_year['2021'].append(i)

        return vars_by_year

    def create_prior_knowledge_matrix(self, var_names: List[str], strictness: str = 'adaptive') -> Tuple[np.ndarray, Dict]:
        """
        Create constraint matrix for DirectLiNGAM (corrected version)
        prior_knowledge[i,j] = 1 means "prohibit j→i"
        """
        n_vars = len(var_names)
        prior_knowledge = np.zeros((n_vars, n_vars))
        vars_by_year = self.classify_vars_by_year(var_names)

        print(f"\nConstraint setting mode: {strictness}")

        # Temporal constraints (prohibit future→past)
        constraint_count = 0

        # Prohibit causality from 2023 to 2021/2022
        for i in vars_by_year['2021'] + vars_by_year['2022']:
            for j in vars_by_year['2023']:
                prior_knowledge[i, j] = 1  # Prohibit 2023→2021/2022
                constraint_count += 1

        # Prohibit causality from 2022 to 2021
        for i in vars_by_year['2021']:
            for j in vars_by_year['2022']:
                prior_knowledge[i, j] = 1  # Prohibit 2022→2021
                constraint_count += 1

        print(f"- Temporal constraints: {constraint_count}")

        if strictness == 'strict':
            # Apply all logical constraints
            logical_count = 0
            applied_constraints = []

            for source_var, target_var in self.logical_constraints:
                if source_var in var_names and target_var in var_names:
                    i = var_names.index(target_var)  # target is i
                    j = var_names.index(source_var)  # source is j
                    if prior_knowledge[i, j] == 0:
                        prior_knowledge[i, j] = 1
                        logical_count += 1
                        if len(applied_constraints) < 5:
                            applied_constraints.append(f"{source_var} → {target_var}")

            print(f"- Logical constraints: {logical_count}")
            if applied_constraints:
                print("  Examples of applied constraints:")
                for constraint in applied_constraints:
                    print(f"    {constraint}")
                if logical_count > 5:
                    print(f"    ... and {logical_count - 5} more")

        elif strictness == 'moderate':
            # Constrain only critical immutable variables
            critical_immutable = ['Male', 'Age_21']
            immutable_count = 0

            for target_var in critical_immutable:
                if target_var in var_names:
                    j_idx = var_names.index(target_var)
                    # Prohibit causality only from variables in the same year
                    for i_idx in range(n_vars):
                        if i_idx != j_idx and prior_knowledge[j_idx, i_idx] == 0:
                            source_var = var_names[i_idx]
                            if source_var in ['Lack_social_support', 'Current_smoking',
                                             'No_health_checkup', 'Low_income', 'Living_alone',
                                             'Poor_self_rated_health', 'Heavy_drinking',
                                             'Low_physical_activity', 'BMI_21',
                                             'Disease_count_21', 'Symptom_count_21']:
                                prior_knowledge[j_idx, i_idx] = 1
                                immutable_count += 1

            print(f"- Constraints to critical immutable variables: {immutable_count} ({critical_immutable} only)")

        elif strictness == 'minimal':
            print("- No additional constraints (temporal constraints only)")

        # Constraint diagnostics
        can_be_effect = np.sum(prior_knowledge == 0, axis=0) - 1
        cant_be_effect_count = np.sum(can_be_effect == -1)
        cant_be_effect_vars = [var_names[i] for i in range(n_vars) if can_be_effect[i] == -1]

        total_constraints = np.sum(prior_knowledge == 1)
        print(f"\nTotal constraints: {total_constraints}")
        print(f"Variables that cannot be effects: {cant_be_effect_count}", end="")
        if cant_be_effect_vars:
            print(f" ({', '.join(cant_be_effect_vars[:5])}", end="")
            if len(cant_be_effect_vars) > 5:
                print(f"... and {len(cant_be_effect_vars) - 5} more", end="")
            print(")")
        else:
            print()

        if cant_be_effect_count >= 3:
            print("⚠️  Warning: 3 or more variables cannot be effects, DirectLiNGAM likely to fail")

        constraint_info = {
            'strictness': strictness,
            'vars_by_year': vars_by_year,
            'total_constraints': total_constraints,
            'cant_be_effect_count': cant_be_effect_count,
            'cant_be_effect_vars': cant_be_effect_vars,
            'logical_constraints': self.logical_constraints
        }

        return prior_knowledge, constraint_info

    def filter_violations(self, causal_matrix: np.ndarray, var_names: List[str],
                         constraint_info: Dict, method_name: str = '') -> np.ndarray:
        """Common function to remove constraint violations (corrected version)"""
        filtered_matrix = causal_matrix.copy()
        vars_by_year = constraint_info['vars_by_year']
        violations = {'temporal': 0, 'logical': 0}

        # Temporal violations (remove future→past)
        # causal_matrix[i,j] is the coefficient for j→i

        # Remove causality from 2023→2021/2022
        for i in vars_by_year['2021'] + vars_by_year['2022']:
            for j in vars_by_year['2023']:
                if abs(filtered_matrix[i, j]) > 0.0001:
                    violations['temporal'] += 1
                    filtered_matrix[i, j] = 0

        # Remove causality from 2022→2021
        for i in vars_by_year['2021']:
            for j in vars_by_year['2022']:
                if abs(filtered_matrix[i, j]) > 0.0001:
                    violations['temporal'] += 1
                    filtered_matrix[i, j] = 0

        # Logical constraint violations
        violated_constraints = []
        for source_var, target_var in self.logical_constraints:
            if source_var in var_names and target_var in var_names:
                i = var_names.index(target_var)
                j = var_names.index(source_var)
                if abs(filtered_matrix[i, j]) > 0.0001:
                    violations['logical'] += 1
                    filtered_matrix[i, j] = 0
                    if len(violated_constraints) < 10:
                        violated_constraints.append(f"{source_var} → {target_var}")

        total_violations = sum(violations.values())
        if total_violations > 0 and method_name:
            print(f"\n{method_name} - Constraint violations removed:")
            if violations['temporal'] > 0:
                print(f"  - Temporal violations: {violations['temporal']}")
            if violations['logical'] > 0:
                print(f"  - Logical constraint violations: {violations['logical']}")
                if violated_constraints:
                    print("    Examples of violated constraints:")
                    for vc in violated_constraints:
                        print(f"      {vc}")
                    if violations['logical'] > 10:
                        print(f"    ... and {violations['logical'] - 10} more")

        return filtered_matrix

    def run_method_cv(self, df: pd.DataFrame, method_name: str, variable_subset: str = 'all') -> Dict:
        """Common function to execute specified method with CV"""
        if self.use_cv:
            print(f"\n{method_name} [{variable_subset}] - Running {self.n_folds}-fold CV...")

            # Data preparation
            data_scaled, var_names = self.prepare_causal_data(df, variable_subset)
            n_samples, n_variables = data_scaled.shape

            # Get variables used in prepare_causal_data and maintain indices
            available_vars = var_names  # Already obtained
            data_for_cv = df[available_vars].dropna()  # Re-obtain from original DataFrame

            # Set up KFold with UserID (to prevent same user from being in multiple folds)
            if 'UserID' in df.columns:
                user_ids = df.loc[data_for_cv.index, 'UserID'].values
                kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
                fold_indices = list(kf.split(user_ids))
            else:
                kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)
                fold_indices = list(kf.split(data_scaled))

            # Save results for each fold
            fold_results = []

            for fold_num, (train_idx, test_idx) in enumerate(fold_indices, 1):
                print(f"\nFold {fold_num}/{self.n_folds}:")

                # Use training data
                train_data = data_scaled[train_idx]

                # Execute according to method
                if method_name == 'DirectLiNGAM':
                    result = self._run_directlingam_single(train_data, var_names, variable_subset)
                elif method_name == 'GOLEM':
                    result = self._run_golem_single(train_data, var_names, variable_subset)
                elif method_name == 'CORL':
                    result = self._run_corl_single(train_data, var_names, variable_subset)
                elif method_name == 'DAGMA':
                    result = self._run_dagma_single(train_data, var_names, variable_subset)

                if result:
                    fold_results.append(result)

            # Integrate CV results
            return self._aggregate_cv_results(fold_results, method_name, variable_subset)

        else:
            # Normal single execution
            if method_name == 'DirectLiNGAM':
                return self.run_directlingam_analysis(df, variable_subset)
            elif method_name == 'GOLEM':
                return self.run_golem_analysis(df, variable_subset)
            elif method_name == 'CORL':
                return self.run_corl_analysis(df, variable_subset)
            elif method_name == 'DAGMA':
                return self.run_dagma_analysis(df, variable_subset)

    def _aggregate_cv_results(self, fold_results: List[Dict], method_name: str, variable_subset: str) -> Dict:
        """Aggregate CV results and extract edges detected in 3+ folds"""
        if not fold_results:
            return None

        # Count edges in all folds
        edge_counts = defaultdict(int)
        edge_coefficients = defaultdict(list)

        for result in fold_results:
            if 'significant_links' in result:
                for link, details in result['significant_links'].items():
                    edge_counts[link] += 1
                    edge_coefficients[link].append(details['coefficient'])

        # Select edges detected in 3+ folds
        significant_links = {}
        threshold_folds = max(3, int(self.n_folds * 0.6))  # Edges detected in 60%+ of folds

        for link, count in edge_counts.items():
            if count >= threshold_folds:
                # Calculate average coefficient
                avg_coef = np.mean(edge_coefficients[link])
                significant_links[link] = {
                    'coefficient': float(avg_coef),
                    'method': method_name,
                    'subset': variable_subset,
                    'fold_count': count,
                    'fold_percentage': count / self.n_folds * 100
                }

        print(f"\n{method_name} [{variable_subset}] CV results:")
        print(f"- Total edges: {len(edge_counts)}")
        print(f"- Detected in {threshold_folds}+ folds: {len(significant_links)}")

        # Get basic information from first result
        var_names = fold_results[0]['var_names'] if fold_results else []

        return {
            'causal_matrix': None,  # Individual matrices not retained during CV
            'significant_links': significant_links,
            'var_names': var_names,
            'method': method_name,
            'subset': variable_subset,
            'cv_info': {
                'n_folds': self.n_folds,
                'threshold_folds': threshold_folds,
                'total_edges_detected': len(edge_counts),
                'significant_edges': len(significant_links)
            }
        }

    def _run_directlingam_single(self, data: np.ndarray, var_names: List[str], variable_subset: str) -> Dict:
        """DirectLiNGAM single execution (for CV)"""
        if not GCASTLE_AVAILABLE:
            return None

        try:
            n_samples, n_variables = data.shape

            # Try from strict to loose constraints
            for strictness in ['strict', 'moderate', 'minimal']:
                prior_knowledge, constraint_info = self.create_prior_knowledge_matrix(
                    var_names, strictness=strictness
                )

                lingam = DirectLiNGAM(
                    prior_knowledge=prior_knowledge,
                    thresh=0.01,
                    measure='pwling'
                )

                try:
                    lingam.learn(data)
                    causal_matrix = lingam.causal_matrix.copy()

                    # Apply post-constraints
                    causal_matrix = self.filter_violations(
                        causal_matrix, var_names, constraint_info, 'DirectLiNGAM'
                    )

                    # Extract significant links
                    significant_links = {}
                    for i in range(len(var_names)):
                        for j in range(len(var_names)):
                            if abs(causal_matrix[i, j]) > 0.001:
                                link_name = f"{var_names[j]} → {var_names[i]}"
                                significant_links[link_name] = {
                                    'coefficient': float(causal_matrix[i, j]),
                                    'method': 'DirectLiNGAM',
                                    'subset': variable_subset
                                }

                    return {
                        'causal_matrix': causal_matrix,
                        'significant_links': significant_links,
                        'var_names': var_names,
                        'method': 'DirectLiNGAM',
                        'subset': variable_subset
                    }

                except ValueError as ve:
                    if "argmax of an empty sequence" in str(ve):
                        continue
                    else:
                        raise ve

            # If failed with all constraint levels
            lingam_no_constraint = DirectLiNGAM(
                prior_knowledge=None,
                thresh=0.01,
                measure='pwling'
            )

            lingam_no_constraint.learn(data)
            causal_matrix = lingam_no_constraint.causal_matrix.copy()

            # Apply post-constraints
            final_constraint_info = {
                'strictness': 'none',
                'vars_by_year': self.classify_vars_by_year(var_names),
                'logical_constraints': self.logical_constraints
            }
            causal_matrix = self.filter_violations(
                causal_matrix, var_names, final_constraint_info, 'DirectLiNGAM'
            )

            # Extract significant links
            significant_links = {}
            for i in range(len(var_names)):
                for j in range(len(var_names)):
                    if abs(causal_matrix[i, j]) > 0.001:
                        link_name = f"{var_names[j]} → {var_names[i]}"
                        significant_links[link_name] = {
                            'coefficient': float(causal_matrix[i, j]),
                            'method': 'DirectLiNGAM',
                            'subset': variable_subset
                        }

            return {
                'causal_matrix': causal_matrix,
                'significant_links': significant_links,
                'var_names': var_names,
                'method': 'DirectLiNGAM',
                'subset': variable_subset
            }

        except Exception as e:
            print(f"DirectLiNGAM execution error: {e}")
            return None

    def _run_golem_single(self, data: np.ndarray, var_names: List[str], variable_subset: str) -> Dict:
        """GOLEM single execution (for CV)"""
        if not GCASTLE_AVAILABLE:
            return None

        try:
            n_samples, n_variables = data.shape

            _, constraint_info = self.create_prior_knowledge_matrix(var_names, strictness='strict')
            vars_by_year = constraint_info['vars_by_year']

            # Reflect constraints in initial values
            B_init = np.random.randn(n_variables, n_variables).astype(np.float32) * 0.1

            # Initialize elements that would be temporal violations to 0
            for i in vars_by_year['2021'] + vars_by_year['2022']:
                for j in vars_by_year['2023']:
                    B_init[i, j] = 0
            for i in vars_by_year['2021']:
                for j in vars_by_year['2022']:
                    B_init[i, j] = 0

            # Also set logical constraints to 0 in initial values
            for source_var, target_var in self.logical_constraints:
                if source_var in var_names and target_var in var_names:
                    i = var_names.index(target_var)
                    j = var_names.index(source_var)
                    B_init[i, j] = 0

            params = {'lr': 2e-3,
                      'lambda_1': 1e-3,
                      'lambda_2': 2.5,
                      'num_iter': 1000}

            golem = GOLEM(
                B_init=B_init,
                lambda_1=params['lambda_1'],
                lambda_2=params['lambda_2'],
                equal_variances=False,
                learning_rate=params['lr'],
                num_iter=params['num_iter'],
                graph_thres=0.001,
                device_type='gpu' if torch.cuda.is_available() else 'cpu',
            )

            golem.learn(data)

            # Post-filtering
            causal_matrix = self.filter_violations(
                golem.causal_matrix, var_names, constraint_info, 'GOLEM'
            )

            # Extract significant links
            significant_links = {}
            for i in range(len(var_names)):
                for j in range(len(var_names)):
                    if causal_matrix[i, j] != 0:
                        link_name = f"{var_names[j]} → {var_names[i]}"
                        significant_links[link_name] = {
                            'coefficient': float(causal_matrix[i, j]),
                            'method': 'GOLEM',
                            'subset': variable_subset
                        }

            return {
                'causal_matrix': causal_matrix,
                'significant_links': significant_links,
                'var_names': var_names,
                'method': 'GOLEM',
                'subset': variable_subset
            }

        except Exception as e:
            print(f"GOLEM execution error: {e}")
            return None

    def _run_corl_single(self, data: np.ndarray, var_names: List[str], variable_subset: str) -> Dict:
        """CORL single execution (for CV)"""
        if not GCASTLE_AVAILABLE:
            return None

        try:
            n_samples, n_variables = data.shape

            # Output debug information
            print(f"Data shape: {data.shape}")
            print(f"Data type: {data.dtype}")
            print(f"Variable list ({len(var_names)}): {var_names[:5]}...")  # Display only first 5

            # Ensure data is converted to float32 and C-contiguous array
            data = np.ascontiguousarray(data, dtype=np.float32)

            # Check for NaN or Inf
            if np.any(np.isnan(data)) or np.any(np.isinf(data)):
                print("Warning: Data contains NaN or Inf")
                data = np.nan_to_num(data, nan=0.0, posinf=1e10, neginf=-1e10).astype(np.float32)

            _, constraint_info = self.create_prior_knowledge_matrix(var_names, strictness='moderate')

            params = {'embed_dim': 64, 'encoder_blocks': 4, 'iteration': 10}

            # CORL initialization (dtype parameter removed)
            corl = CORL(
                batch_size=32,
                input_dim=32,
                embed_dim=params['embed_dim'],
                encoder_name='transformer',
                encoder_heads=4,
                encoder_blocks=params['encoder_blocks'],
                encoder_dropout_rate=0.1,
                decoder_name='lstm',
                reward_mode='episodic',
                reward_score_type='BIC_different_var',
                reward_regression_type='LR',
                lambda_iter_num=500,
                actor_lr=0.0001,
                critic_lr=0.001,
                iteration=int(params['iteration']),
                device_type='gpu' if torch.cuda.is_available() else 'cpu'
            )

            # Set PyTorch default tensor type (temporarily)
            original_dtype = torch.get_default_dtype()
            torch.set_default_dtype(torch.float32)

            try:
                corl.learn(data)
            finally:
                # Restore original default type
                torch.set_default_dtype(original_dtype)

            # Post-filtering
            causal_matrix = self.filter_violations(
                corl.causal_matrix, var_names, constraint_info, 'CORL'
            )

            # Extract significant links
            significant_links = {}
            threshold = 0.001
            for i in range(len(var_names)):
                for j in range(len(var_names)):
                    if i != j and abs(causal_matrix[i, j]) > threshold:
                        link_name = f"{var_names[j]} → {var_names[i]}"
                        significant_links[link_name] = {
                            'coefficient': float(causal_matrix[i, j]),
                            'method': 'CORL',
                            'subset': variable_subset
                        }

            return {
                'causal_matrix': causal_matrix,
                'significant_links': significant_links,
                'var_names': var_names,
                'method': 'CORL',
                'subset': variable_subset
            }

        except Exception as e:
            print(f"CORL execution error: {e}")
            print(f"Variable subset: {variable_subset}")
            print(f"Number of variables: {len(var_names) if var_names else 'N/A'}")
            import traceback
            traceback.print_exc()
            return None

    def _run_dagma_single(self, data: np.ndarray, var_names: List[str], variable_subset: str) -> Dict:
        """DAGMA single execution (for CV)"""
        if not DAGMA_AVAILABLE:
            return None

        try:
            n_samples, n_variables = data.shape
            data = np.ascontiguousarray(data, dtype=np.float64)

            _, constraint_info = self.create_prior_knowledge_matrix(var_names, strictness='Strict')

            # Convert data to Torch tensor and send to device
            X_torch = torch.from_numpy(data).to(self.device)

            # Create DagmaMLP according to DAGMA documentation/source code
            # DagmaMLP dims are [input dimension, hidden layer 1, ..., output dimension]
            # Each output node is 1 dimensional, so output dimension is 1
            eq_model = DagmaMLP(
                dims=[n_variables, 10, 1],  # [n_variables, hidden layer dimension, output dimension (usually 1)]
                bias=True,                  # bias=True is default and more stable, False tends to cause errors
            )

            # Create DagmaNonlinear instance
            # DagmaNonlinear constructor takes 'model' and 'dtype'
            model = DagmaNonlinear(
                model=eq_model,
            )

            # Re-adjust parameters based on original code values
            # Keep only arguments that can be passed directly to fit method
            dagma_fit_params = {
                'lambda1': 1e-3,
                'lambda2': 0.001,
                'lr': 0.0003,
                'w_threshold': 0.001,
                'T': 4,
                'mu_init': 0.1,
                'mu_factor': 0.1,
                's': 1,
                'warm_iter': 500,
                'max_iter': 800,
                'checkpoint': 1000
            }

            W_est = model.fit(
                data,  # Pass numpy array directly
                **dagma_fit_params
                )

            # DAGMA needs transpose, then filter
            # W_est is returned as numpy array on CPU, so use directly
            causal_matrix = self.filter_violations(
                W_est.T, var_names, constraint_info, 'DAGMA'
            )

            # Extract significant links
            significant_links = {}
            threshold = 0.001
            for i in range(len(var_names)):
                for j in range(len(var_names)):
                    if i != j and abs(causal_matrix[i, j]) > threshold:
                        link_name = f"{var_names[j]} → {var_names[i]}"
                        significant_links[link_name] = {
                            'coefficient': float(causal_matrix[i, j]),
                            'method': 'DAGMA',
                            'subset': variable_subset
                        }

            return {
                'causal_matrix': causal_matrix,
                'significant_links': significant_links,
                'var_names': var_names,
                'method': 'DAGMA',
                'subset': variable_subset
            }

        except Exception as e:
            print(f"DAGMA execution error: {e}")
            return None

    def run_directlingam_analysis(self, df: pd.DataFrame, variable_subset: str = 'all') -> Dict:
        """DirectLiNGAM analysis (CV compatible)"""
        return self.run_method_cv(df, 'DirectLiNGAM', variable_subset)

    def run_golem_analysis(self, df: pd.DataFrame, variable_subset: str = 'all') -> Dict:
        """GOLEM analysis (CV compatible)"""
        return self.run_method_cv(df, 'GOLEM', variable_subset)

    def run_corl_analysis(self, df: pd.DataFrame, variable_subset: str = 'all') -> Dict:
        """CORL analysis (CV compatible)"""
        return self.run_method_cv(df, 'CORL', variable_subset)

    def run_dagma_analysis(self, df: pd.DataFrame, variable_subset: str = 'all') -> Dict:
        """DAGMA analysis (CV compatible)"""
        return self.run_method_cv(df, 'DAGMA', variable_subset)

    def integrate_results(self, df: pd.DataFrame) -> pd.DataFrame:
        """Integrate results from 4 methods (CV compatible version)"""
        print("\nIntegrating causal inference results...")

        if self.use_cv:
            print(f"CV setting: {self.n_folds}-fold CV, threshold: significant in {max(3, int(self.n_folds * 0.6))}+ folds")

        # Execute methods for each group
        groups = ['disease_infection', 'symptoms']
        all_results = {}

        for group in groups:
            print(f"\n{'='*60}")
            group_display_name = {
                'disease_infection': 'Disease & Infection',
                'symptoms': 'Symptoms'
            }[group]
            print(f"Running causal inference for {group_display_name} group...")
            print(f"{'='*60}")

            # Execute each method
            corl_results = self.run_corl_analysis(df, group)
            lingam_results = self.run_directlingam_analysis(df, group)
            golem_results = self.run_golem_analysis(df, group)
            dagma_results = self.run_dagma_analysis(df, group)

            all_results[group] = {
                'corl': corl_results,
                'lingam': lingam_results,
                'golem': golem_results,
                'dagma': dagma_results
            }

        # Collect all links
        all_links = {}

        for group in groups:
            group_results = all_results[group]

            for method_name, method_results in group_results.items():
                if method_results and 'significant_links' in method_results:
                    for link, details in method_results['significant_links'].items():
                        if link not in all_links:
                            all_links[link] = {
                                'CORL': '-',
                                'DirectLiNGAM': '-',
                                'GOLEM': '-',
                                'DAGMA': '-',
                                'group': group,
                                'fold_counts': {}
                            }

                        method_col_name = {
                            'corl': 'CORL',
                            'lingam': 'DirectLiNGAM',
                            'golem': 'GOLEM',
                            'dagma': 'DAGMA'
                        }[method_name]

                        all_links[link][method_col_name] = '✓'
                        all_links[link][f'{method_col_name}_coef'] = details['coefficient']

                        # Save CV information
                        if 'fold_count' in details:
                            all_links[link]['fold_counts'][method_col_name] = details['fold_count']

        # Create integrated results
        integrated_results = []

        for link, detection_info in sorted(all_links.items()):
            result = {
                'Path': link,
                'Group': detection_info['group'],
                'CORL': detection_info['CORL'],
                'DirectLiNGAM': detection_info['DirectLiNGAM'],
                'GOLEM': detection_info['GOLEM'],
                'DAGMA': detection_info['DAGMA']
            }

            # Consensus level
            detected_count = sum([result[method] == '✓' for method in ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']])
            if detected_count >= 4:
                result['Consensus'] = 'All four'
            elif detected_count >= 3:
                result['Consensus'] = 'Three methods'
            elif detected_count == 2:
                result['Consensus'] = 'Two methods'
            else:
                result['Consensus'] = 'One method'

            # Average coefficient
            coefficients = []
            for method in ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']:
                coef_key = f'{method}_coef'
                if coef_key in detection_info:
                    coefficients.append(detection_info[coef_key])

            if coefficients:
                result['Avg_Coefficient'] = np.mean(coefficients)
            else:
                result['Avg_Coefficient'] = 0

            # Add CV information
            if self.use_cv and detection_info['fold_counts']:
                cv_info = []
                for method in ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']:
                    if method in detection_info['fold_counts']:
                        cv_info.append(f"{method}:{detection_info['fold_counts'][method]}")
                result['CV_Info'] = ', '.join(cv_info) if cv_info else '-'

            integrated_results.append(result)

        # Save results
        self.results = all_results
        self.results['integrated'] = pd.DataFrame(integrated_results)

        # Display summary
        print(f"\n{'='*60}")
        print("Causal Inference Results Summary:")
        print(f"{'='*60}")

        # Display HI variable type
        print(f"\nHI variable type: {self.hi_var_type}")
        hi_var_used = 'High_HI (Binary)' if self.hi_var_type == 'binary' else 'HealthIndifferenceScore (Continuous)'
        print(f"HI variable used: {hi_var_used}")

        if self.use_cv:
            print(f"\nCV settings:")
            print(f"- Number of folds: {self.n_folds}")
            print(f"- Significance threshold: {max(3, int(self.n_folds * 0.6))}+ folds")

        # Results by method
        for group in groups:
            group_display_name = {
                'disease_infection': 'Disease & Infection',
                'symptoms': 'Symptoms'
            }[group]
            print(f"\n{group_display_name} group:")
            group_results = all_results[group]
            for method_name, method_results in group_results.items():
                if method_results:
                    n_links = len(method_results.get('significant_links', {}))
                    method_display_name = {
                        'corl': 'CORL',
                        'lingam': 'DirectLiNGAM',
                        'golem': 'GOLEM',
                        'dagma': 'DAGMA'
                    }[method_name]

                    cv_info_str = ""
                    if 'cv_info' in method_results:
                        cv_info = method_results['cv_info']
                        cv_info_str = f" (CV: {cv_info['significant_edges']}/{cv_info['total_edges_detected']} edges passed threshold)"

                    print(f"- {method_display_name}: {n_links} links detected{cv_info_str}")

        print(f"\nIntegrated results:")
        print(f"- Total detected links: {len(all_links)}")
        print(f"- Disease & Infection group links: {len([l for l in all_links.values() if l['group'] == 'disease_infection'])}")
        print(f"- Symptoms group links: {len([l for l in all_links.values() if l['group'] == 'symptoms'])}")

        return pd.DataFrame(integrated_results)

# ========================================
# 10. Longitudinal Analysis
# ========================================
class LongitudinalAnalyzer:
    """Execute longitudinal analysis (mutually adjusted model)"""

    @staticmethod
    def run_modified_poisson_regression(df: pd.DataFrame, outcome: str,
                                      exposure: str, covariates: List[str]) -> Dict:
        """Modified Poisson regression analysis (single exposure variable)"""
        vars_needed = [outcome, exposure] + covariates
        missing_vars = [v for v in vars_needed if v not in df.columns]

        if missing_vars:
            print(f"Warning: Missing variables in {outcome} model: {missing_vars}")
            return None

        data = df[vars_needed].dropna()

        if len(data) < 50:
            print(f"Warning: Insufficient sample size for {outcome} model (n={len(data)})")
            return None

        try:
            X = sm.add_constant(data[[exposure] + covariates])
            y = data[outcome]

            # Modified Poisson regression (using robust standard errors)
            model = sm.GLM(y, X, family=sm.families.Poisson()).fit(cov_type='HC0')

            rr_val = np.exp(model.params[exposure])
            ci = np.exp(model.conf_int().loc[exposure])
            p_val = model.pvalues[exposure]

            return {
                'outcome': outcome,
                'n': len(data),
                'rr': rr_val,
                'ci_lower': ci[0],
                'ci_upper': ci[1],
                'p_value': p_val,
                'converged': model.converged
            }
        except Exception as e:
            print(f"Error in {outcome} model: {e}")
            return None


    @staticmethod
    def calculate_vif(df: pd.DataFrame, variables: List[str]) -> Dict[str, float]:
        """Calculate VIF (Variance Inflation Factor)"""
        data = df[variables].dropna()

        vif_dict = {}
        X = data.values

        for i, var in enumerate(variables):
            try:
                vif = variance_inflation_factor(X, i)
                vif_dict[var] = vif
            except:
                vif_dict[var] = np.nan

        return vif_dict

    @staticmethod
    def run_modified_poisson_regression_mutual_adjustment(df: pd.DataFrame,
                                                        outcome: str,
                                                        determinants: List[str]) -> Dict:
        """Mutually adjusted modified Poisson regression (for determinant analysis)"""
        vars_needed = [outcome] + determinants
        missing_vars = [v for v in vars_needed if v not in df.columns]

        if missing_vars:
            print(f"Warning: Missing variables in {outcome} model: {missing_vars}")
            return None

        data = df[vars_needed].dropna()

        if len(data) < 50:
            print(f"Warning: Insufficient sample size for {outcome} model (n={len(data)})")
            return None

        try:
            X = sm.add_constant(data[determinants])
            y = data[outcome]

            # Modified Poisson regression (using robust standard errors)
            model = sm.GLM(y, X, family=sm.families.Poisson()).fit(cov_type='HC0')

            results = {}
            for det in determinants:
                if det in model.params.index:
                    rr_val = np.exp(model.params[det])
                    ci = np.exp(model.conf_int().loc[det])
                    p_val = model.pvalues[det]

                    results[det] = {
                        'rr': rr_val,
                        'ci_lower': ci[0],
                        'ci_upper': ci[1],
                        'p_value': p_val
                    }

            return {
                'model': model,
                'n': len(data),
                'results': results,
                'converged': model.converged
            }
        except Exception as e:
            print(f"Error in {outcome} model: {e}")
            return None

    @staticmethod
    def create_table2_determinants(df: pd.DataFrame) -> pd.DataFrame:
        """Table 2: Determinants of health indifference (mutually adjusted model + VIF)"""
        results = []

        print("\nAnalyzing determinants of health indifference (mutually adjusted model, modified Poisson regression)...")

        if 'HI_category' in df.columns:
            df['High_HI'] = (df['HI_category'] == 'High HI').astype(int)

            all_determinants = [
                ('Male', 'Male sex'),
                ('Low_education', 'Low education (<high school)'),
                ('Lack_social_support', 'Lack of social support'),
                ('Current_smoking', 'Current smoking'),
                ('No_health_checkup', 'No health checkup'),
                ('Low_income', 'Low household income'),
                ('Living_alone', 'Living alone'),
                ('Heavy_drinking', 'Heavy drinking'),
                ('Low_physical_activity', 'Low physical activity'),
                ('Poor_self_rated_health', 'Poor self-rated health'),
                ('Age_21', 'Age (years)')
            ]

            available_determinants = [det[0] for det in all_determinants if det[0] in df.columns]

            # Calculate VIF
            print("Calculating VIF (Variance Inflation Factor)...")
            vif_values = LongitudinalAnalyzer.calculate_vif(df, available_determinants)

            # Execute mutually adjusted modified Poisson regression
            model_result = LongitudinalAnalyzer.run_modified_poisson_regression_mutual_adjustment(
                df, 'High_HI', available_determinants
            )

            if model_result and model_result['results']:
                for var, label in all_determinants:
                    if var in model_result['results']:
                        result = model_result['results'][var]
                        vif = vif_values.get(var, np.nan)

                        results.append({
                            'Variable': label,
                            'Mutually Adjusted RR (95% CI)':
                                f"{result['rr']:.2f} ({result['ci_lower']:.2f}-{result['ci_upper']:.2f})",
                            'p-value': '<0.001' if result['p_value'] < 0.001 else f"{result['p_value']:.3f}",
                            'n': model_result['n'],
                            'VIF': f"{vif:.2f}" if not np.isnan(vif) else "NA"
                        })

        return pd.DataFrame(results)

    @staticmethod
    def create_table3_outcomes(df: pd.DataFrame) -> pd.DataFrame:
        """Table 3: Health outcomes associated with health indifference (adjusted for all determinants)"""
        results = []

        print("\nAnalyzing health outcomes (adjusted for all determinants, modified Poisson regression)...")

        if 'HI_category' in df.columns:
            df['High_HI'] = (df['HI_category'] == 'High HI').astype(int)

            # Covariate list
            all_covariates_2021 = ['Age_21', 'Male', 'Low_education', 'Low_income',
                                   'Living_alone', 'Lack_social_support',
                                   'Current_smoking', 'Heavy_drinking',
                                   'Low_physical_activity', 'No_health_checkup',
                                   'Poor_self_rated_health', 'Disease_count_21',
                                   'Symptom_count_21', 'BMI_21']

            available_covariates = [c for c in all_covariates_2021 if c in df.columns]

            # All health outcomes (2023)
            all_outcomes = [
                # Diseases
                ('HospitalizationPastYear_23', 'Hospitalization (past year)'),
                ('Hypertension_23_current', 'Hypertension'),
                ('AnginaOrMI_23_current', 'Angina/MI'),
                ('Stroke_23_current', 'Stroke'),
                ('Cancer_23_current', 'Cancer'),
                ('Diabetes_23_current', 'Diabetes mellitus'),
                ('Dyslipidemia_23_current', 'Dyslipidemia'),
                ('Depression_23_current', 'Depression'),
                ('Periodontitis_23_current', 'Periodontitis'),
                ('DentalCaries_23_current', 'Dental caries'),
                ('Asthma_23_current', 'Asthma'),
                ('ChronicObstructivePulmonaryDisease_23_current', 'COPD'),
                ('ChronicKidneyDisease_23_current', 'Chronic kidney disease'),
                ('ChronicLiverDisease_23_current', 'Chronic liver disease'),
                ('ChronicPain_23_current', 'Chronic pain'),

                # Symptoms
                ('Fatigue_23_moderate', 'Fatigue'),
                ('SleepDisorder_23_moderate', 'Sleep disturbance'),
                ('BackPain_23_moderate', 'Back pain'),
                ('DigestiveDisorder_23_moderate', 'GI discomfort'),
                ('Headache_23_moderate', 'Headache'),
                ('JointPain_23_moderate', 'Joint pain'),
                ('ConcentrationDecline_23_moderate', 'Concentration decline'),
                ('MemoryImpairment_23_moderate', 'Memory disorder'),
                ('ChestPain_23_moderate', 'Chest pain'),
                ('Cough_23_moderate', 'Cough'),
                ('Dyspnea_23_moderate', 'Dyspnea'),
                ('Dizziness_23_moderate', 'Dizziness'),
                ('ReducedLibido_23_moderate', 'Reduced libido'),
                ('Fever_23_moderate', 'Fever'),

                # Infection-related
                ('COVID19_infection_within_1year_23', 'COVID-19 infection (past year)'),
                ('COVID19_vaccinated_23', 'COVID-19 vaccination'),
                ('Influenza_vaccinated_23', 'Influenza vaccination'),
            ]

            for outcome, label in all_outcomes:
                if outcome in df.columns:
                    result = LongitudinalAnalyzer.run_modified_poisson_regression(
                        df, outcome, 'High_HI', available_covariates
                    )

                    if result:
                        results.append({
                            'Variable': label,
                            'Adjusted RR (95% CI)':
                                f"{result['rr']:.2f} ({result['ci_lower']:.2f}-{result['ci_upper']:.2f})",
                            'p-value': '<0.001' if result['p_value'] < 0.001 else f"{result['p_value']:.3f}",
                            'n': result['n']
                        })

        return pd.DataFrame(results)

# ========================================
# 11. Visualization Functions
# ========================================
class Visualizer:
    """Create publication-quality figures"""

    @staticmethod
    def save_figure(filename: str):
        """Save figure to Google Drive and locally with Autodesk Graphic Standards"""
        local_path = filename

        if DRIVE_MOUNTED and DRIVE_OUTPUT_DIR:
            drive_path = os.path.join(DRIVE_OUTPUT_DIR, filename)
            shutil.copy2(local_path, drive_path)
            print(f"  → Google Drive saved: {drive_path}")

        print(f"  → Local saved: {local_path}")

    @staticmethod
    def create_participant_flowchart(df: pd.DataFrame, output_path: str = 'figure1_participant_flow_binary.png'):
        """Figure 1: Participant flowchart (dynamic data version)"""
        fig, ax = plt.subplots(figsize=(10, 12))
        ax.axis('off')

        # Calculate numbers from actual data
        total_n = len(df)
        hi_valid = df['HealthIndifferenceScore'].notna().sum() if 'HealthIndifferenceScore' in df.columns else 0
        low_hi_n = (df['HI_category'] == 'Low HI').sum() if 'HI_category' in df.columns else 0
        high_hi_n = (df['HI_category'] == 'High HI').sum() if 'HI_category' in df.columns else 0

        # Calculate exclusions
        hi_missing = total_n - hi_valid
        low_hi_pct = (low_hi_n / hi_valid * 100) if hi_valid > 0 else 0
        high_hi_pct = (high_hi_n / hi_valid * 100) if hi_valid > 0 else 0

        # Define boxes
        boxes = [
            {'text': f'Baseline Survey (2021)\nn = {total_n:,}',
             'xy': (0.5, 0.9), 'color': 'lightblue'},
            {'text': f'Excluded:\n- Missing HI scale: n = {hi_missing:,}',
             'xy': (0.8, 0.7), 'color': 'lightcoral'},
            {'text': f'Complete HI scale data\nn = {hi_valid:,}',
             'xy': (0.5, 0.5), 'color': 'lightblue'},
            {'text': f'Final Analytical Cohort\nn = {hi_valid:,}\n\nLow HI: n = {low_hi_n:,} ({low_hi_pct:.1f}%)\nHigh HI: n = {high_hi_n:,} ({high_hi_pct:.1f}%)',
             'xy': (0.5, 0.2), 'color': 'lightgreen'}
        ]

        for box in boxes:
            ax.text(box['xy'][0], box['xy'][1], box['text'],
                   transform=ax.transAxes, fontsize=12,
                   verticalalignment='center', horizontalalignment='center',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor=box['color'], alpha=0.8))

        arrows = [
            ((0.5, 0.85), (0.5, 0.55)),
            ((0.5, 0.85), (0.7, 0.7)),
            ((0.5, 0.45), (0.5, 0.25))
        ]

        for start, end in arrows:
            ax.annotate('', xy=end, xytext=start,
                       xycoords='axes fraction', textcoords='axes fraction',
                       arrowprops=dict(arrowstyle='->', lw=2, color='gray'))

        plt.title('Figure 1. Participant Flow Diagram', fontsize=16, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        Visualizer.save_figure(output_path)

    @staticmethod
    def create_forest_plot(table_df: pd.DataFrame, plot_type: str, output_path: str):
        """Create forest plot (for Table2 or Table3)"""
        fig, ax = plt.subplots(figsize=(10, 12))

        outcomes = []

        if table_df is not None and not table_df.empty:
            for _, row in table_df.iterrows():
                try:
                    rr_col_name = None
                    if 'Mutually Adjusted RR (95% CI)' in row:
                        rr_col_name = 'Mutually Adjusted RR (95% CI)'
                    elif 'Adjusted RR (95% CI)' in row:
                        rr_col_name = 'Adjusted RR (95% CI)'

                    if rr_col_name:
                        rr_ci = row[rr_col_name]
                        rr_match = re.match(r'([\d.]+)\s*\(([\d.]+)-([\d.]+)\)', rr_ci)

                        if rr_match:
                            rr_val = float(rr_match.group(1))
                            ci_low = float(rr_match.group(2))
                            ci_high = float(rr_match.group(3))

                            outcomes.append({
                                'name': row['Variable'],
                                'rr': rr_val,
                                'ci_low': ci_low,
                                'ci_high': ci_high,
                                'p_value': row['p-value']
                            })
                except Exception as e:
                    print(f"Warning: Error extracting RR value for {row['Variable']}: {e}")
                    continue

        if outcomes:
            outcomes.sort(key=lambda x: x['rr'], reverse=True)

            y_pos = np.arange(len(outcomes))

            for i, outcome in enumerate(outcomes):
                ax.plot([outcome['ci_low'], outcome['ci_high']], [i, i], 'b-', linewidth=2)

                try:
                    p_val = float(outcome['p_value']) if outcome['p_value'] != '<0.001' else 0.0001
                    color = 'blue' if p_val < 0.05 else 'gray'
                except:
                    color = 'gray'
                ax.plot(outcome['rr'], i, 'o', color=color, markersize=8)

                ci_text = f"{outcome['rr']:.2f} ({outcome['ci_low']:.2f}-{outcome['ci_high']:.2f})"
                ax.text(outcome['ci_high'] + 0.02, i, ci_text, va='center', fontsize=8)

            ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7)

            ax.set_yticks(y_pos)
            ax.set_yticklabels([o['name'] for o in outcomes], fontsize=10)
            ax.set_xlabel('Risk Ratio (95% CI)', fontsize=12)

        if plot_type == 'determinants':
            title = 'Forest Plot: Determinants of Health Indifference'
            subtitle = '(Mutually adjusted risk ratios for High HI vs Low HI)'
        else:
            title = 'Forest Plot: Health Outcomes Associated with Health Indifference'
            subtitle = '(Risk ratios adjusted for baseline characteristics)'

        plt.title(f'{title}\n{subtitle}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        Visualizer.save_figure(output_path)

    @staticmethod
    def create_forest_plots(table2_df: pd.DataFrame, table3_df: pd.DataFrame):
        """Create both Figure 2A and 2B forest plots"""
        if table2_df is not None and not table2_df.empty:
            Visualizer.create_forest_plot(
                table2_df,
                'determinants',
                'figure2a_forest_plot_determinants_binary.png'
            )

        if table3_df is not None and not table3_df.empty:
            Visualizer.create_forest_plot(
                table3_df,
                'outcomes',
                'figure2b_forest_plot_outcomes_binary.png'
            )

    @staticmethod
    def create_causal_dag(causal_results: Dict, output_path: str = 'figure3_causal_dag_binary.png'):
        """Figure 3: Split DAG based on causal inference results into two (supporting HI variable types)"""

        def create_dag_subset(edges_with_details: Dict, all_vars: set,
                            subset_type: str, fig_num: str, hi_var_type: str, use_cv: bool) -> None:
            """Create DAG subset"""
            fig, ax = plt.subplots(figsize=(36, 24))

            # Create NetworkX graph
            G = nx.DiGraph()

            # Classify variables by time and type
            vars_2021 = []
            vars_2022 = []
            vars_2023_target = []
            vars_2023_other = []

            # Determine HI variable name
            hi_var_name = 'High_HI' if hi_var_type == 'binary' else 'HealthIndifferenceScore'

            # Variable classification
            for var in all_vars:
                if var in ['HealthIndifferenceScore', 'High_HI']:
                    vars_2022.append(var)
                elif any(suffix in var for suffix in ['_23', '_current', '_moderate', 'PastYear_23']):
                    if subset_type == 'disease_infection':
                        if ('_current' in var or 'HospitalizationPastYear_23' in var or
                            'COVID19' in var or 'Influenza' in var):
                            vars_2023_target.append(var)
                        else:
                            vars_2023_other.append(var)
                    else:  # symptoms
                        if '_moderate' in var:
                            vars_2023_target.append(var)
                        else:
                            vars_2023_other.append(var)
                else:
                    vars_2021.append(var)

            # Sort
            vars_2021.sort()
            vars_2022.sort()
            vars_2023_target.sort()
            vars_2023_other.sort()

            # Select only edges relevant to this subset
            relevant_edges = {}
            for edge, details in edges_with_details.items():
                if details.get('group') == subset_type:
                    relevant_edges[edge] = details

            # Calculate node positions
            pos = {}

            y_spacing_2021 = 18.0
            y_spacing_2023 = 18.0
            x_spacing = 15

            # 2021 variables (left side)
            y_offset_2021 = len(vars_2021) * y_spacing_2021 / 2
            for i, var in enumerate(vars_2021):
                pos[var] = (0, y_offset_2021 - i * y_spacing_2021)

            # 2022 variables (center)
            y_center = 0
            for i, var in enumerate(vars_2022):
                pos[var] = (x_spacing, y_center + i * 3)

            # 2023 target variables (right side)
            y_offset_2023 = len(vars_2023_target) * y_spacing_2023 / 2
            for i, var in enumerate(vars_2023_target):
                pos[var] = (x_spacing * 2, y_offset_2023 - i * y_spacing_2023)

            # Other 2023 variables
            for var in vars_2023_other:
                pos[var] = (x_spacing * 2.5, 0)

            # Add nodes to graph
            display_nodes = vars_2021 + vars_2022 + vars_2023_target
            G.add_nodes_from(display_nodes)

            # Add edges
            for edge in relevant_edges:
                if edge[0] in display_nodes and edge[1] in display_nodes:
                    G.add_edge(edge[0], edge[1])

            # Draw nodes
            node_colors = []
            node_sizes = []
            for node in G.nodes():
                if node in vars_2021:
                    node_colors.append('lightblue')
                    node_sizes.append(6000)
                elif node in vars_2022:
                    node_colors.append('yellow')
                    node_sizes.append(8000)
                elif node in vars_2023_target:
                    node_colors.append('lightgreen')
                    node_sizes.append(6000)

            nx.draw_networkx_nodes(G, pos, node_color=node_colors,
                                   node_size=node_sizes, ax=ax, alpha=0.9,
                                   edgecolors='black', linewidths=2)

            # Create node labels (according to HI variable type)
            labels = {}
            for node in G.nodes():
                if node == 'HealthIndifferenceScore':
                    labels[node] = 'Health Indifference'
                elif node == 'High_HI':
                    labels[node] = 'Health Indifference'
                elif '_23_current' in node:
                    labels[node] = node.replace('_23_current', '').replace('_', ' ')
                elif '_23_moderate' in node:
                    labels[node] = node.replace('_23_moderate', '').replace('_', ' ')
                elif 'HospitalizationPastYear_23' in node:
                    labels[node] = 'Hospitalization'
                elif 'COVID19_infection_within_1year_23' in node:
                    labels[node] = 'COVID-19\nInfection'
                elif 'COVID19_vaccinated_23' in node:
                    labels[node] = 'COVID-19\nVaccination'
                elif 'Influenza_vaccinated_23' in node:
                    labels[node] = 'Influenza\nVaccination'
                elif '_21' in node:
                    labels[node] = node.replace('_21', '').replace('_', ' ')
                else:
                    labels[node] = node.replace('_', ' ')

                # Line break for long names
                if len(labels[node]) > 15 and '\n' not in labels[node]:
                    words = labels[node].split()
                    if len(words) > 1:
                        mid = len(words) // 2
                        labels[node] = ' '.join(words[:mid]) + '\n' + ' '.join(words[mid:])

            nx.draw_networkx_labels(G, pos, labels, font_size=14, ax=ax)

            # Draw edges (color-blind friendly palette)
            base_colors = {
                'All four': '#D55E00',
                'Three methods': '#0072B2',
                'Two methods': '#CC79A7',
                'CORL': '#E69F00',
                'DirectLiNGAM': '#56B4E9',
                'GOLEM': '#009E73',
                'DAGMA': '#661100',
            }

            light_colors = {
                'All four': '#EAA585',
                'Three methods': '#7AB8D8',
                'Two methods': '#E0B9CE',
                'CORL': '#F2CE80',
                'DirectLiNGAM': '#ABD2F4',
                'GOLEM': '#80CEB9',
                'DAGMA': '#A67F66',
            }

            # Draw edges
            for edge, details in relevant_edges.items():
                if edge[0] in display_nodes and edge[1] in display_nodes:
                    methods = details['methods']
                    hi_related = details['hi_related']
                    avg_coef = details['avg_coefficient']

                    # Determine base color according to method combination
                    if len(methods) == 4:
                        base_color_key = 'All four'
                        width = 8
                    elif len(methods) == 3:
                        base_color_key = 'Three methods'
                        width = 6.5
                    elif len(methods) == 2:
                        base_color_key = 'Two methods'
                        width = 5
                    elif len(methods) == 1:
                        base_color_key = methods[0]
                        width = 3.5
                    else:
                        base_color_key = None
                        width = 2.0

                    # Determine color and style based on whether HI-related
                    if base_color_key:
                        if hi_related:
                            color = base_colors[base_color_key]
                            style = 'solid'
                            alpha = 0.95
                        else:
                            color = light_colors[base_color_key]
                            style = 'dashed'
                            alpha = 0.3
                    else:
                        color = '#C0C0C0'
                        style = 'dashed'
                        alpha = 0.3

                    # Draw edge
                    nx.draw_networkx_edges(G, pos, edgelist=[edge], ax=ax,
                                        edge_color=color, width=width,
                                        style=style, alpha=alpha,
                                        arrows=True, arrowsize=25,
                                        arrowstyle='->')

            # Add year labels
            if display_nodes:
                y_max = max([p[1] for p in pos.values() if p[0] <= x_spacing * 2])
                ax.text(0, y_max + 10, '2021',
                        fontsize=22, fontweight='bold', ha='center')
                ax.text(x_spacing, y_max + 10, '2022',
                        fontsize=22, fontweight='bold', ha='center')
                ax.text(x_spacing * 2, y_max + 10, '2023',
                        fontsize=22, fontweight='bold', ha='center')

            # Create legend
            from matplotlib.lines import Line2D
            from matplotlib.patches import Patch

            custom_lines = []
            line_labels = []

            # Consensus level
            custom_lines.append(Line2D([0], [0], color=base_colors['All four'], lw=12, label='All four methods'))
            line_labels.append('All four methods')

            custom_lines.append(Line2D([0], [0], color=base_colors['Three methods'], lw=10, label='Three methods'))
            line_labels.append('Three methods')

            custom_lines.append(Line2D([0], [0], color=base_colors['Two methods'], lw=8, label='Two methods'))
            line_labels.append('Two methods')

            # Single method
            custom_lines.append(Line2D([0], [0], color=base_colors['CORL'], lw=6, label='CORL only'))
            line_labels.append('CORL only')

            custom_lines.append(Line2D([0], [0], color=base_colors['DirectLiNGAM'], lw=6, label='DirectLiNGAM only'))
            line_labels.append('DirectLiNGAM only')

            custom_lines.append(Line2D([0], [0], color=base_colors['GOLEM'], lw=6, label='GOLEM only'))
            line_labels.append('GOLEM only')

            custom_lines.append(Line2D([0], [0], color=base_colors['DAGMA'], lw=6, label='DAGMA only'))
            line_labels.append('DAGMA only')

            # HI relevance
            custom_lines.append(Line2D([0], [0], color='#000000', lw=6, linestyle='-', label='HI related path'))
            line_labels.append('HI related path')

            custom_lines.append(Line2D([0], [0], color='#808080', lw=6, linestyle='--', label='Non-HI related path'))
            line_labels.append('Non-HI related path')

            # Patches
            custom_patches = [
                Patch(facecolor='lightblue', edgecolor='black', linewidth=2),
                Patch(facecolor='yellow', edgecolor='black', linewidth=2),
                Patch(facecolor='lightgreen', edgecolor='black', linewidth=2)
            ]

            # Position legends
            legend1 = ax.legend(custom_lines, line_labels,
                              loc='upper left',
                              bbox_to_anchor=(0.80, 0.95),
                              fontsize=24,
                              frameon=True,
                              fancybox=True,
                              shadow=True,
                              title='Edge Types',
                              title_fontsize=26)

            # Node label according to HI variable type
            if hi_var_type == 'binary':
                node_label_2022 = '2022 Health Indifference (Binary)'
            else:
                node_label_2022 = '2022 Health Indifference (Continuous)'

            patch_labels = ['2021 Determinants', node_label_2022, '2023 Outcomes']
            legend2 = ax.legend(custom_patches, patch_labels,
                              loc='upper left',
                              bbox_to_anchor=(0.80, 0.35),
                              fontsize=24,
                              frameon=True,
                              fancybox=True,
                              shadow=True,
                              title='Node Types',
                              title_fontsize=26,
                              handlelength=3,
                              handleheight=2)

            ax.add_artist(legend1)

            # Set title (specifying HI variable type and CV information)
            hi_var_desc = 'High HI (Binary)' if hi_var_type == 'binary' else 'HI Score (Continuous)'
            cv_desc = ' (5-fold CV validated)' if use_cv else ''

            if subset_type == 'disease_infection':
                title = f'Causal DAG - Disease & Infection Outcomes{cv_desc}\n'
                title += f'{hi_var_desc} → Chronic Diseases & Infection-related Outcomes (2021-2023)'
            else:
                title = f'Causal DAG - Symptom Outcomes{cv_desc}\n'
                title += f'{hi_var_desc} → Physical & Mental Symptoms (2021-2023)'

            ax.set_title(title, fontsize=22, fontweight='bold', pad=30)
            ax.axis('off')

            # Adjust axis range
            x_margin = 10
            y_margin = 15
            display_pos = {k: v for k, v in pos.items() if k in display_nodes}
            if display_pos:
                x_values = [p[0] for p in display_pos.values()]
                y_values = [p[1] for p in display_pos.values()]
                ax.set_xlim(min(x_values) - x_margin, max(x_values) + x_margin + 15)
                ax.set_ylim(min(y_values) - y_margin, max(y_values) + y_margin + 5)

            # Set filename
            cv_suffix = '_cv' if use_cv else ''
            if subset_type == 'disease_infection':
                save_path = output_path.replace('_binary.png', f'_disease_infection{cv_suffix}_binary.png')
            else:
                save_path = output_path.replace('_binary.png', f'_symptoms{cv_suffix}_binary.png')

            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()

            Visualizer.save_figure(save_path)

        # Main processing
        all_vars = set()
        edges_with_details = {}
        hi_var_type = 'continuous'  # Default value
        use_cv = False  # Default value

        if causal_results and 'integrated' in causal_results:
            integrated_df = causal_results['integrated']

            # Determine HI variable type and CV usage
            if 'causal_analyzer' in causal_results:
                hi_var_type = causal_results['causal_analyzer'].hi_var_type
                use_cv = causal_results['causal_analyzer'].use_cv

            # Extract variables and edges from all paths
            for _, row in integrated_df.iterrows():
                path = row['Path']
                if ' → ' in path:
                    source, target = path.split(' → ')
                    all_vars.add(source)
                    all_vars.add(target)

                    # Record which methods detected it
                    methods_detected = []
                    if row.get('CORL', '-') == '✓':
                        methods_detected.append('CORL')
                    if row.get('DirectLiNGAM', '-') == '✓':
                        methods_detected.append('DirectLiNGAM')
                    if row.get('GOLEM', '-') == '✓':
                        methods_detected.append('GOLEM')
                    if row.get('DAGMA', '-') == '✓':
                        methods_detected.append('DAGMA')

                    # Determine if HI-related (supporting both HI variable types)
                    hi_related = ('HealthIndifferenceScore' in source or
                                'HealthIndifferenceScore' in target or
                                'High_HI' in source or
                                'High_HI' in target)

                    edges_with_details[(source, target)] = {
                        'consensus': row['Consensus'],
                        'methods': methods_detected,
                        'hi_related': hi_related,
                        'avg_coefficient': row.get('Avg_Coefficient', 0),
                        'group': row.get('Group', 'unknown')
                    }

        # Create two figures
        print("Creating Figure 3 split into two (group-specific execution results)...")
        create_dag_subset(edges_with_details, all_vars, 'disease_infection', '3', hi_var_type, use_cv)
        create_dag_subset(edges_with_details, all_vars, 'symptoms', '3', hi_var_type, use_cv)
        cv_info = " (CV validated)" if use_cv else ""
        print(f"Figure 3 (Disease & Infection) and Figure 4 (Symptoms) created{cv_info}")

    @staticmethod
    def create_score_distribution(df: pd.DataFrame, output_path: str = 'supp_figure1_distribution_binary.png'):
        """Supplementary Figure 1: HI score distribution (dynamic version)"""
        if 'HealthIndifferenceScore' not in df.columns:
            print("Warning: HealthIndifferenceScore not found")
            return

        fig, ax = plt.subplots(figsize=(10, 6))

        scores = df['HealthIndifferenceScore'].dropna()

        if len(scores) == 0:
            print("Warning: No valid scores")
            ax.text(0.5, 0.5, 'No valid scores found',
                    ha='center', va='center', transform=ax.transAxes, fontsize=14)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
        else:
            n_bins = min(40, max(10, int(np.sqrt(len(scores)))))

            n, bins, patches = ax.hist(scores, bins=n_bins, density=True, alpha=0.7,
                                      color='skyblue', edgecolor='navy')

            mean_score = scores.mean()
            median_score = scores.median()
            std_score = scores.std()

            ax.axvline(mean_score, color='red', linestyle='--', linewidth=2,
                      label=f'Mean = {mean_score:.1f}')
            ax.axvline(median_score, color='green', linestyle='--', linewidth=2,
                      label=f'Median = {median_score:.0f}')

            # Normal distribution fit
            from scipy.stats import norm
            xmin, xmax = scores.min() - std_score, scores.max() + std_score
            x = np.linspace(xmin, xmax, 100)
            p = norm.pdf(x, mean_score, std_score)
            ax.plot(x, p, 'k', linewidth=2, label='Normal fit')

            # Statistics
            skewness = scores.skew()
            kurtosis = scores.kurtosis()

            textstr = f'n = {len(scores):,}\nMean = {mean_score:.1f}\nSD = {std_score:.1f}\nSkewness = {skewness:.2f}\nKurtosis = {kurtosis:.2f}'
            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,
                    verticalalignment='top', bbox=props)

            ax.set_xlabel('Health Indifference Score', fontsize=12)
            ax.set_ylabel('Density', fontsize=12)
            ax.set_xlim(xmin, xmax)
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.title(f'Distribution of Health Indifference Scores',
                 fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        Visualizer.save_figure(output_path)

    @staticmethod
    def create_causal_inference_details(causal_results: Dict, output_path: str = 'supp_figure2_causal_details_binary.png'):
        """Supplementary Figure 2: Detailed visualization of causal inference results (supporting HI variable types and CV)"""
        if not causal_results or 'integrated' not in causal_results:
            print("Warning: Causal inference results not available")
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Determine HI variable type and CV usage
        hi_var_type = 'continuous'
        use_cv = False
        if 'causal_analyzer' in causal_results:
            hi_var_type = causal_results['causal_analyzer'].hi_var_type
            use_cv = causal_results['causal_analyzer'].use_cv

        # 1. Number of detected links by method
        ax1 = axes[0, 0]

        integrated_df = causal_results['integrated']
        method_names = ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']

        counts = []
        for method in method_names:
            count = (integrated_df[method] == '✓').sum()
            counts.append(count)

        colors = ['#FF8C00', '#1E90FF', '#228B22', '#9400D3']

        bars = ax1.bar(method_names, counts, color=colors)
        ax1.set_title('Number of Detected Links by Method' + (' (CV validated)' if use_cv else ''))
        ax1.set_ylabel('Number of Links')
        ax1.tick_params(axis='x', rotation=45)

        for bar, count in zip(bars, counts):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
                    f'{count}', ha='center', va='bottom')

        # 2. Consensus level distribution
        ax2 = axes[0, 1]
        consensus_counts = integrated_df['Consensus'].value_counts()
        if not consensus_counts.empty:
            consensus_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=plt.cm.Pastel1.colors)
            ax2.set_title('Consensus Level Distribution')
            ax2.set_ylabel('')

        # 3. Coefficient distribution
        ax3 = axes[1, 0]

        if 'Avg_Coefficient' in integrated_df.columns:
            coefficients = integrated_df['Avg_Coefficient'].dropna()
            if len(coefficients) > 0:
                n_bins = min(20, max(5, int(np.sqrt(len(coefficients)))))
                ax3.hist(coefficients, bins=n_bins, color='skyblue', edgecolor='navy', alpha=0.7)
                ax3.axvline(coefficients.mean(), color='red', linestyle='--',
                           label=f'Mean = {coefficients.mean():.3f}')
                ax3.set_title('Distribution of Average Coefficients')
                ax3.set_xlabel('Average Coefficient')
                ax3.set_ylabel('Frequency')
                ax3.legend()
            else:
                ax3.text(0.5, 0.5, 'No coefficient data available',
                        ha='center', va='center', transform=ax3.transAxes)

        # 4. Table of major causal paths (with CV information)
        ax4 = axes[1, 1]
        ax4.axis('off')

        # Prioritize HI-related paths (supporting both HI variable types)
        hi_related = integrated_df[
            integrated_df['Path'].str.contains('HealthIndifferenceScore') |
            integrated_df['Path'].str.contains('High_HI')
        ]

        if len(hi_related) > 0:
            # Sort by consensus level
            consensus_order = {'All four': 4, 'Three methods': 3, 'Two methods': 2, 'One method': 1, 'None': 0}
            hi_related['consensus_score'] = hi_related['Consensus'].map(consensus_order)

            if 'Avg_Coefficient' in hi_related.columns:
                hi_related['abs_coefficient'] = hi_related['Avg_Coefficient'].abs()
                sorted_df = hi_related.sort_values(['consensus_score', 'abs_coefficient'],
                                                 ascending=[False, False])
            else:
                sorted_df = hi_related.sort_values('consensus_score', ascending=False)

            max_display = min(10, len(sorted_df))
            display_paths = sorted_df.head(max_display)

            table_data = []
            headers = ['HI-Related Paths', 'Consensus', 'Avg Coef']
            if use_cv and 'CV_Info' in display_paths.columns:
                headers.append('CV Info')

            for _, row in display_paths.iterrows():
                path = row['Path']
                # Shorten path (supporting HI variable types)
                path = path.replace('HealthIndifferenceScore', 'HI Score')
                path = path.replace('High_HI', 'High HI')
                path = path.replace('_23_current', '')
                path = path.replace('_23_moderate', '')
                path = path.replace('_21', '')

                if len(path) > 30:
                    path = path[:27] + '...'

                if 'Avg_Coefficient' in row:
                    coef_str = f"{row['Avg_Coefficient']:.3f}"
                else:
                    coef_str = "N/A"

                row_data = [path, row['Consensus'], coef_str]

                if use_cv and 'CV_Info' in row:
                    cv_info = row['CV_Info'] if row['CV_Info'] != '-' else 'N/A'
                    row_data.append(cv_info)

                table_data.append(row_data)

            # Display HI variable type and CV information
            hi_var_desc = 'High HI (Binary)' if hi_var_type == 'binary' else 'HI Score (Continuous)'
            cv_desc = ' (5-fold CV)' if use_cv else ''

            table = ax4.table(cellText=table_data,
                            colLabels=headers,
                            cellLoc='left',
                            loc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(8)
            table.scale(1, 1.5)
            ax4.set_title(f'Top {max_display} HI-Related Consensus Paths\n({hi_var_desc}{cv_desc})', y=0.95)
        else:
            ax4.text(0.5, 0.5, 'No HI-related paths found',
                    ha='center', va='center', transform=ax4.transAxes)

        cv_info_str = ' (5-fold CV)' if use_cv else ''
        plt.suptitle(f'Causal Inference Analysis Details{cv_info_str}\n(HI Variable: {hi_var_type})',
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        Visualizer.save_figure(output_path)

# ========================================
# 12. E-value Calculation
# ========================================
def calculate_e_value(or_value: float, ci_lower: float = None, ci_upper: float = None) -> Tuple[float, float, float]:
    """Calculate E-value (supporting both upper and lower confidence limits)"""
    # E-value for point estimate
    if or_value < 1:
        inv_or = 1 / or_value if or_value > 0 else float('inf')
        e_value = inv_or + np.sqrt(inv_or * (inv_or - 1))
    else:
        e_value = or_value + np.sqrt(or_value * (or_value - 1))

    # E-value for lower confidence limit
    if ci_lower is not None:
        if or_value < 1:
            if ci_upper is not None and ci_upper > 0:
                inv_ci = 1 / ci_upper
                e_value_ci_lower = inv_ci + np.sqrt(inv_ci * (inv_ci - 1)) if inv_ci > 1 else 1
            else:
                e_value_ci_lower = 1
        else:
            e_value_ci_lower = ci_lower + np.sqrt(ci_lower * (ci_lower - 1)) if ci_lower > 1 else 1
    else:
        e_value_ci_lower = None

    # E-value for upper confidence limit
    if ci_upper is not None:
        if or_value < 1:
            if ci_lower is not None and ci_lower > 0:
                inv_ci = 1 / ci_lower
                e_value_ci_upper = inv_ci + np.sqrt(inv_ci * (inv_ci - 1)) if inv_ci > 1 else float('inf')
            else:
                e_value_ci_upper = float('inf')
        else:
            e_value_ci_upper = ci_upper + np.sqrt(ci_upper * (ci_upper - 1))
    else:
        e_value_ci_upper = None

    return e_value, e_value_ci_lower, e_value_ci_upper

# ========================================
# 13. Main Analysis Pipeline
# ========================================
class JASTISAnalysisPipeline:
    """Main analysis pipeline (HI variable type selectable, CV compatible)"""

    def __init__(self, data_path: str, hi_var_type: str = 'continuous', use_cv: bool = True):
        """
        Parameters
        ----------
        data_path : str
            Data file path
        hi_var_type : str
            'continuous': HealthIndifferenceScore (continuous value 13-52)
            'binary': High_HI (binary variable 0/1)
        use_cv : bool
            True: Execute 5-fold CV
            False: Traditional single execution
        """
        self.data_path = data_path
        self.hi_var_type = hi_var_type
        self.use_cv = use_cv
        self.df = None
        self.imputed_datasets = None
        self.results = {}
        self.hi_calculator = None

    def run_complete_analysis(self):
        """Execute complete analysis pipeline"""
        print("="*60)
        print("JASTIS Health Indifference Study - Analysis Pipeline (5-fold CV Version)")
        print("="*60)
        print("DirectLiNGAM constraints corrected")
        print("Causal inference methods: CORL, DirectLiNGAM, GOLEM, DAGMA")
        print(f"HI variable type: {self.hi_var_type}")
        print(f"Cross-Validation: {'5-fold CV' if self.use_cv else 'Single execution'}")
        if self.use_cv:
            print("Significance criterion: Edges detected in 3+ folds")
        print("Statistical method: Modified Poisson regression (robust standard errors)")
        print("="*60)

        print("\n[Step 1] Loading and validating data...")
        self.df = load_and_validate_data(self.data_path)

        print("\n[Step 2] Calculating health indifference score...")
        self.hi_calculator = HealthIndifferenceCalculator()
        self.df = self.hi_calculator.calculate_score(self.df)

        print("\n[Step 3] Creating analysis variables...")
        self.df = VariableCreator.create_all_variables(self.df)

        print("\n[Step 4] Handling missing data...")
        missing_handler = MissingDataHandler()
        missing_info = missing_handler.analyze_missing_patterns(self.df)
        print(f"Complete cases: {missing_info['pct_complete_cases']:.1f}%")

        print("\n[Step 5] Creating Table 1...")
        table1 = StatisticalAnalyzer.create_table1(self.df)
        print("\nTable 1. Participant Characteristics by Health Indifference Status")
        print("=" * 100)
        print(table1.to_string(index=False))
        self.results['table1'] = table1

        print("\n[Step 6] Longitudinal analysis (mutually adjusted model)...")

        # Table 2: Determinants of health indifference
        table2 = LongitudinalAnalyzer.create_table2_determinants(self.df)
        print("\nTable 2. Determinants of Health Indifference (Mutual Adjustment Model)")
        print("=" * 100)
        print(table2.to_string(index=False))
        self.results['table2'] = table2

        # Table 3: Health outcomes
        table3 = LongitudinalAnalyzer.create_table3_outcomes(self.df)
        print("\nTable 3. Health Outcomes Associated with Health Indifference")
        print("=" * 100)
        print(table3.to_string(index=False))
        self.results['table3'] = table3

        print("\n[Step 7] Causal inference analysis...")
        if self.use_cv:
            print("Executing causal inference with 5-fold Cross-Validation...")

        if GCASTLE_AVAILABLE or DAGMA_AVAILABLE:
            causal_analyzer = CausalInferenceAnalyzer(
                hi_var_type=self.hi_var_type,
                use_cv=self.use_cv,
                n_folds=5
            )
            causal_results = causal_analyzer.integrate_results(self.df)
            self.results['causal_results'] = causal_results
            self.results['causal_analyzer'] = causal_analyzer

            if not causal_results.empty:
                print("\nCausal Inference Results")
                if self.use_cv:
                    print("(5-fold CV: Edges detected in ≥3 folds)")
                print("="*120)

                # Add number of detection methods for each row
                causal_results['method_count'] = causal_results.apply(
                    lambda row: sum([row[m] == '✓' for m in ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']]),
                    axis=1
                )

                # Group by number of methods
                for n_methods in [4, 3, 2, 1]:
                    group_data = causal_results[causal_results['method_count'] == n_methods].copy()

                    if len(group_data) > 0:
                        if n_methods == 4:
                            print(f"\n【All four methods】 ({len(group_data)} paths)")
                        elif n_methods == 3:
                            print(f"\n【Three methods】 ({len(group_data)} paths)")
                        elif n_methods == 2:
                            print(f"\n【Two methods】 ({len(group_data)} paths)")
                        else:
                            print(f"\n【One method】 ({len(group_data)} paths)")
                        print("-"*120)

                        # Display data
                        display_columns = ['Path', 'CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']
                        if 'Avg_Coefficient' in group_data.columns:
                            display_columns.append('Avg_Coefficient')
                        if self.use_cv and 'CV_Info' in group_data.columns:
                            display_columns.append('CV_Info')

                        if 'Avg_Coefficient' in group_data.columns:
                            group_data['abs_coef'] = group_data['Avg_Coefficient'].abs()
                            group_data_sorted = group_data.sort_values('abs_coef', ascending=False)
                        else:
                            group_data_sorted = group_data.sort_values('Path')

                        with pd.option_context('display.max_rows', None, 'display.width', 200):
                            print(group_data_sorted[display_columns].to_string(index=False))

                # Overall statistics
                print("\n" + "="*120)
                print("Summary Statistics")
                print("="*120)

                print(f"\nTotal causal paths detected: {len(causal_results)}")
                if self.use_cv:
                    print(f"(All paths passed ≥3 folds validation)")

                # Detection count by method
                print("\nDetection by method:")
                for method in ['CORL', 'DirectLiNGAM', 'GOLEM', 'DAGMA']:
                    count = (causal_results[method] == '✓').sum()
                    percentage = count / len(causal_results) * 100
                    print(f"  {method}: {count} paths ({percentage:.1f}%)")

                # HI-related statistics (supporting both HI variable types)
                hi_var_name = 'High_HI' if self.hi_var_type == 'binary' else 'HealthIndifferenceScore'
                hi_as_source = causal_results[causal_results['Path'].str.startswith(f'{hi_var_name} →')]
                hi_as_target = causal_results[causal_results['Path'].str.endswith(f'→ {hi_var_name}')]
                hi_related = causal_results[causal_results['Path'].str.contains(hi_var_name)]

                print(f"\nHI-related Statistics (HI Variable: {hi_var_name}):")
                print(f"  HI as source: {len(hi_as_source)} paths")
                print(f"  HI as target: {len(hi_as_target)} paths")
                print(f"  Total HI-related: {len(hi_related)} paths ({len(hi_related)/len(causal_results)*100:.1f}%)")

        else:
            print("Warning: Causal inference libraries not available")

        print("\n[Step 8] Creating visualizations...")
        visualizer = Visualizer()
        visualizer.create_participant_flowchart(self.df)

        if 'table2' in self.results and 'table3' in self.results:
            visualizer.create_forest_plots(self.results['table2'], self.results['table3'])

        if 'causal_analyzer' in self.results:
            # Pass including causal_analyzer
            causal_results_with_analyzer = self.results['causal_analyzer'].results.copy()
            causal_results_with_analyzer['causal_analyzer'] = self.results['causal_analyzer']
            visualizer.create_causal_dag(causal_results_with_analyzer)
            visualizer.create_causal_inference_details(causal_results_with_analyzer)

        visualizer.create_score_distribution(self.df)

        print("\n[Step 9] Calculating E-values...")
        self.calculate_e_values()

        print("\n[Step 10] Saving results...")
        self.save_results()

        print("\n" + "="*60)
        print("Analysis complete!")
        if self.use_cv:
            print("Causal inference with 5-fold CV completed")
        print("="*60)

    def calculate_e_values(self):
        """Calculate E-values for major associations"""
        print("\nE-values for major associations:")
        print("-" * 60)

        e_values = []

        if 'table3' in self.results and not self.results['table3'].empty:
            outcomes_data = self.results['table3']

            for _, row in outcomes_data.iterrows():
                try:
                    rr_ci_str = row['Adjusted RR (95% CI)']
                    rr_match = re.match(r'([\d.]+)\s*\(([\d.]+)-([\d.]+)\)', rr_ci_str)

                    if rr_match:
                        rr_val = float(rr_match.group(1))
                        ci_low = float(rr_match.group(2))
                        ci_high = float(rr_match.group(3))

                        e_val, e_val_ci_lower, e_val_ci_upper = calculate_e_value(rr_val, ci_low, ci_high)

                        if rr_val < 1:
                            effect_type = "Protective effect"
                        else:
                            effect_type = "Risk increase"

                        print(f"{row['Variable']}: RR = {rr_val:.2f} ({ci_low:.2f}-{ci_high:.2f}), "
                              f"E-value = {e_val:.2f} (CI: {e_val_ci_lower:.2f}-{e_val_ci_upper:.2f})")

                        e_values.append({
                            'Outcome': row['Variable'],
                            'RR': rr_val,
                            'CI_lower': ci_low,
                            'CI_upper': ci_high,
                            'E-value': e_val,
                            'E-value_CI_lower': e_val_ci_lower,
                            'E-value_CI_upper': e_val_ci_upper,
                            'Effect': effect_type
                        })
                except Exception as e:
                    print(f"Warning: Error calculating E-value for {row['Variable']}: {e}")
                    continue

        if e_values:
            self.results['e_values'] = pd.DataFrame(e_values)
            self.results['e_values'] = self.results['e_values'].sort_values('RR', ascending=False)
        else:
            print("No valid results available for E-value calculation.")

    def save_results(self):
        """Save all results"""
        os.makedirs('results', exist_ok=True)

        # Save Excel file
        hi_var_suffix = '_binary' if self.hi_var_type == 'binary' else '_continuous'
        cv_suffix = '_cv' if self.use_cv else ''
        excel_path = f'results/JASTIS_analysis_results{hi_var_suffix}{cv_suffix}.xlsx'

        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            if 'table1' in self.results:
                self.results['table1'].to_excel(writer, sheet_name='Table1_Characteristics', index=False)
            if 'table2' in self.results:
                self.results['table2'].to_excel(writer, sheet_name='Table2_Determinants', index=False)
            if 'table3' in self.results:
                self.results['table3'].to_excel(writer, sheet_name='Table3_Outcomes', index=False)
            if 'causal_results' in self.results:
                self.results['causal_results'].to_excel(writer, sheet_name='Causal_Inference', index=False)
            if 'e_values' in self.results:
                self.results['e_values'].to_excel(writer, sheet_name='E_values', index=False)

            # Meta information
            meta_df = pd.DataFrame({
                'Parameter': ['HI Variable Type', 'Cross-Validation', 'Analysis Date', 'Sample Size', 'Cronbachs Alpha'],
                'Value': [
                    self.hi_var_type,
                    '5-fold CV' if self.use_cv else 'Single run',
                    datetime.now().strftime('%Y-%m-%d %H:%M'),
                    len(self.df),
                    getattr(self.hi_calculator, 'cronbachs_alpha', 'NA')
                ]
            })
            meta_df.to_excel(writer, sheet_name='Meta_Information', index=False)

        # Create summary report
        self.create_summary_report()

        # Copy results to Google Drive
        if DRIVE_MOUNTED and DRIVE_OUTPUT_DIR:
            print("\nSaving results to Google Drive...")
            shutil.copy2(excel_path, os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(excel_path)))
            print(f"  → Excel file saved: {DRIVE_OUTPUT_DIR}/{os.path.basename(excel_path)}")

            summary_path = f'results/analysis_summary{hi_var_suffix}{cv_suffix}.txt'
            shutil.copy2(summary_path, os.path.join(DRIVE_OUTPUT_DIR, os.path.basename(summary_path)))
            print(f"  → Summary report saved: {DRIVE_OUTPUT_DIR}/{os.path.basename(summary_path)}")

            if os.path.exists('missing_pattern_binary.png'):
                shutil.copy2('missing_pattern_binary.png', os.path.join(DRIVE_OUTPUT_DIR, 'missing_pattern_binary.png'))

        print("\nResults saved to the following files:")
        print(f"- {excel_path}")
        print(f"- results/analysis_summary{hi_var_suffix}{cv_suffix}.txt")
        print("- Various figures (figure*_binary.png, supp_figure*_binary.png)")

        if DRIVE_MOUNTED:
            print(f"\n【Google Drive】{DRIVE_OUTPUT_DIR}/")
            print("  All files have also been copied to Google Drive")

    def create_summary_report(self):
        """Create text format summary report"""
        hi_var_suffix = '_binary' if self.hi_var_type == 'binary' else '_continuous'
        cv_suffix = '_cv' if self.use_cv else ''

        with open(f'results/analysis_summary{hi_var_suffix}{cv_suffix}.txt', 'w', encoding='utf-8') as f:
            f.write("JASTIS Health Indifference Study - Analysis Summary (5-fold CV Version)\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
            f.write(f"Data file: {self.data_path}\n")
            f.write(f"Total sample size: {len(self.df):,}\n")
            f.write(f"HI variable type: {self.hi_var_type}\n")
            f.write(f"Cross-Validation: {'5-fold CV' if self.use_cv else 'Single execution'}\n")

            hi_var_desc = 'High_HI (Binary variable)' if self.hi_var_type == 'binary' else 'HealthIndifferenceScore (Continuous variable)'
            f.write(f"HI variable used: {hi_var_desc}\n")

            f.write("\nDirectLiNGAM constraint correction: Applied (correctly set j→i direction)\n")
            f.write("Causal inference methods: CORL, DirectLiNGAM, GOLEM, DAGMA\n")
            f.write("Statistical method: Modified Poisson regression (robust standard errors)\n")

            if self.use_cv:
                f.write("Significance criterion: Edges detected in 3+ folds\n")

            if hasattr(self.hi_calculator, 'cronbachs_alpha') and self.hi_calculator.cronbachs_alpha is not None:
                f.write(f"\nCronbach's α for health indifference scale: {self.hi_calculator.cronbachs_alpha:.3f}\n")

            if 'HI_category' in self.df.columns:
                low_hi_n = (self.df['HI_category']=='Low HI').sum()
                high_hi_n = (self.df['HI_category']=='High HI').sum()
                f.write(f"\nLow HI: {low_hi_n:,}\n")
                f.write(f"High HI: {high_hi_n:,}\n\n")

            f.write("Major findings:\n")
            f.write("-" * 40 + "\n")

            # 1. Determinants of health indifference
            f.write("1. Determinants of health indifference:\n")
            if 'table2' in self.results and not self.results['table2'].empty:
                determinants_data = self.results['table2']

                significant_determinants = []
                for _, row in determinants_data.iterrows():
                    try:
                        p_val = row['p-value']
                        if p_val == '<0.001' or (isinstance(p_val, str) and float(p_val) < 0.05):
                            significant_determinants.append(row)
                    except:
                        continue

                if significant_determinants:
                    # RR column name corrected
                    significant_determinants.sort(
                        key=lambda x: float(x['Mutually Adjusted RR (95% CI)'].split(' ')[0]),
                        reverse=True
                    )

                    for i, row in enumerate(significant_determinants[:5]):
                        f.write(f"   - {row['Variable']}: RR = {row['Mutually Adjusted RR (95% CI)']} (p {row['p-value']})")
                        if 'VIF' in row:
                            f.write(f", VIF = {row['VIF']}")
                        f.write("\n")
                else:
                    f.write("   - No significant determinants detected\n")
            else:
                f.write("   - Analysis results not available\n")

            f.write("\n")

            # 2. Health outcomes associated with health indifference
            f.write("2. Health outcomes associated with health indifference:\n")
            if 'table3' in self.results and not self.results['table3'].empty:
                outcomes_data = self.results['table3']

                significant_outcomes = []
                for _, row in outcomes_data.iterrows():
                    try:
                        p_val = row['p-value']
                        if p_val == '<0.001' or (isinstance(p_val, str) and float(p_val) < 0.05):
                            significant_outcomes.append(row)
                    except:
                        continue

                if significant_outcomes:
                    # RR column name corrected
                    significant_outcomes.sort(
                        key=lambda x: float(x['Adjusted RR (95% CI)'].split(' ')[0]),
                        reverse=True
                    )

                    for i, row in enumerate(significant_outcomes[:5]):
                        f.write(f"   - {row['Variable']}: RR = {row['Adjusted RR (95% CI)']} (p {row['p-value']})\n")
                else:
                    f.write("   - No significant outcomes detected\n")
            else:
                f.write("   - Analysis results not available\n")

            f.write("\n")

            # 3. Causal inference summary
            f.write("3. Causal inference analysis:\n")
            if self.use_cv:
                f.write("   (Results from 5-fold CV)\n")

            if 'causal_results' in self.results and not self.results['causal_results'].empty:
                corl_count = (self.results['causal_results']['CORL'] == '✓').sum()
                lingam_count = (self.results['causal_results']['DirectLiNGAM'] == '✓').sum()
                golem_count = (self.results['causal_results']['GOLEM'] == '✓').sum()
                dagma_count = (self.results['causal_results']['DAGMA'] == '✓').sum()

                f.write(f"   - CORL: {corl_count} links detected\n")
                f.write(f"   - DirectLiNGAM: {lingam_count} links detected\n")
                f.write(f"   - GOLEM: {golem_count} links detected\n")
                f.write(f"   - DAGMA: {dagma_count} links detected\n")

                if self.use_cv:
                    f.write("   (All links detected in 3+ folds)\n")

                # Consensus distribution
                consensus_counts = self.results['causal_results']['Consensus'].value_counts()
                f.write("\n   Consensus distribution:\n")
                for consensus, count in consensus_counts.items():
                    f.write(f"   - {consensus}: {count} links\n")
            else:
                f.write("   - Causal inference results not available\n")

# ========================================
# 14. Execution Function
# ========================================
def main(hi_var_type: str = 'continuous', use_cv: bool = True):
    """
    Main execution function

    Parameters
    ----------
    hi_var_type : str
        'continuous': HealthIndifferenceScore (continuous value 13-52)
        'binary': High_HI (binary variable 0/1)
    use_cv : bool
        True: Execute 5-fold CV
        False: Traditional single execution
    """
    data_path = 'Jastis_Analysis2.csv'

    if not os.path.exists(data_path):
        print(f"\nError: Data file '{data_path}' not found!")
        print("Please upload the data file or update the path.")

        if IN_COLAB:
            print("\nTo upload a file in Google Colab:")
            print("from google.colab import files")
            print("uploaded = files.upload()")

        return

    # Check library availability
    if not GCASTLE_AVAILABLE:
        print("\nWarning: gCastle not available.")
        print("Installation:")
        print("  pip install gcastle")

    if not DAGMA_AVAILABLE:
        print("\nWarning: DAGMA not available.")
        print("Installation:")
        print("  pip install dagma")

    if not GCASTLE_AVAILABLE and not DAGMA_AVAILABLE:
        print("\nCannot execute causal inference analysis.")
        print("Continue analysis? (y/n): ", end="")
        response = input().strip().lower()
        if response != 'y':
            print("Analysis cancelled.")
            return

    # Execute pipeline
    pipeline = JASTISAnalysisPipeline(data_path, hi_var_type=hi_var_type, use_cv=use_cv)
    pipeline.run_complete_analysis()

if __name__ == "__main__":
    # Detect Jupyter/Colab environment
    try:
        get_ipython()
        IN_JUPYTER = True
    except NameError:
        IN_JUPYTER = False

    if IN_JUPYTER:
        # Interactive selection in Jupyter/Colab environment
        print("Select HI variable type:")
        print("1. continuous (HealthIndifferenceScore: continuous value 13-52)")
        print("2. binary (High_HI: binary variable 0/1)")

        choice = input("Choice (1 or 2): ").strip()
        hi_var_type = 'binary' if choice == '2' else 'continuous'

        print("\nUse Cross-Validation?")
        print("1. Yes (5-fold CV) - Recommended, about 30-60 minutes")
        print("2. No (Single execution) - Fast, about 10-20 minutes")

        cv_choice = input("Choice (1 or 2): ").strip()
        use_cv = (cv_choice == '1')

    else:
        # Use argparse in normal Python environment
        import argparse

        parser = argparse.ArgumentParser(description='JASTIS Health Indifference Study - Causal Inference Analysis')
        parser.add_argument('--hi_var_type', type=str, default='continuous',
                            choices=['continuous', 'binary'],
                            help='HI variable type (continuous: continuous value, binary: binary)')
        parser.add_argument('--use_cv', action='store_true', default=False,
                            help='Use 5-fold CV')
        parser.add_argument('--no_cv', dest='use_cv', action='store_false',
                            help='Do not use CV (single execution)')

        args = parser.parse_args()
        hi_var_type = args.hi_var_type
        use_cv = args.use_cv

    # Display execution information
    print("\nAnalysis settings:")
    print(f"- HI variable type: {hi_var_type}")
    print(f"- Cross-Validation: {'5-fold CV' if use_cv else 'Single execution'}")

    if use_cv:
        print("\nNote: 5-fold CV execution takes time (about 30-60 minutes)")
        print("Continue? (y/n): ", end="")
        response = input().strip().lower()
        if response != 'y':
            print("Analysis cancelled.")
            exit() if not IN_JUPYTER else None

    main(hi_var_type=hi_var_type, use_cv=use_cv)
